{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLPWOGeipZJXXyO9BCXuIb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quadragonsaurus/ArielACE/blob/main/NekoFi_SuperLazy_Cafe_TI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FxddEaEIvGuw",
        "outputId": "56dd9823-cc15-41d0-f613-25c9a7777aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading...\n",
            "From: https://huggingface.co/andite/models/resolve/main/nai-wd.ckpt\n",
            "To: /content/sd_text_inversion/nai-wd.ckpt\n",
            "100% 4.27G/4.27G [01:19<00:00, 53.4MB/s]\n",
            "/content\n",
            "mkdir: cannot create directory ‘sd_text_inversion’: File exists\n",
            "/content/sd_text_inversion\n",
            "/content/sd_text_inversion\n",
            "Cloning into 'stable-textual-inversion-cafe'...\n",
            "remote: Enumerating objects: 706, done.\u001b[K\n",
            "remote: Counting objects: 100% (703/703), done.\u001b[K\n",
            "remote: Compressing objects: 100% (295/295), done.\u001b[K\n",
            "remote: Total 706 (delta 409), reused 650 (delta 384), pack-reused 3\u001b[K\n",
            "Receiving objects: 100% (706/706), 4.35 MiB | 24.61 MiB/s, done.\n",
            "Resolving deltas: 100% (409/409), done.\n",
            "/content/sd_text_inversion/stable-textual-inversion-cafe\n",
            "Done.\n",
            "/content/sd_text_inversion/stable-textual-inversion-cafe\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining taming-transformers from git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
            "  Cloning https://github.com/CompVis/taming-transformers.git (to revision master) to ./src/taming-transformers\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/CompVis/taming-transformers.git /content/sd_text_inversion/stable-textual-inversion-cafe/src/taming-transformers\n",
            "  Resolved https://github.com/CompVis/taming-transformers.git to commit 3ba01b241669f5ade541ce990f7650a3b8f65318\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Obtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip\n",
            "  Cloning https://github.com/openai/CLIP.git (to revision main) to ./src/clip\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /content/sd_text_inversion/stable-textual-inversion-cafe/src/clip\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning==1.6.5\n",
            "  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 KB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting test-tube\n",
            "  Downloading test_tube-0.7.5.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.0-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia\n",
            "  Downloading kornia-0.6.10-py2.py3-none-any.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.0/612.0 KB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.5) (2023.3.0)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.5) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.5) (4.5.0)\n",
            "Collecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.5) (4.65.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.5) (23.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.5) (3.19.6)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.5) (2.11.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.5) (1.22.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.6.5) (6.0)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.9/dist-packages (from test-tube) (1.4.4)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.9/dist-packages (from test-tube) (2.9.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from test-tube) (0.16.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip) (0.14.1+cu116)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from imageio>=2.3.0->test-tube) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.20.3->test-tube) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.20.3->test-tube) (2022.7.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.16.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.38.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (63.4.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.4.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.51.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip) (0.2.6)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (22.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (5.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.2.2)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, test-tube\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=eb8f65bbec4fc099b7c2df1052313402c322a9f0fa1a8c8f47c2987964c82248\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "  Building wheel for test-tube (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for test-tube: filename=test_tube-0.7.5-py3-none-any.whl size=25356 sha256=f5f1c526f6e5a8cbcd449ef60ab969b45530d09bc5f46952aacec711c447d9b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/58/3f/e21fc7e325685fffc4b9b866f3be18d7a208f34ff5f847f3d5\n",
            "Successfully built antlr4-python3-runtime test-tube\n",
            "Installing collected packages: tokenizers, antlr4-python3-runtime, pyDeprecate, omegaconf, multidict, ftfy, frozenlist, einops, charset-normalizer, async-timeout, yarl, torchmetrics, taming-transformers, kornia, huggingface-hub, aiosignal, transformers, clip, aiohttp, test-tube, pytorch-lightning\n",
            "  Running setup.py develop for taming-transformers\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 async-timeout-4.0.2 charset-normalizer-3.1.0 clip-1.0 einops-0.6.0 frozenlist-1.3.3 ftfy-6.1.1 huggingface-hub-0.13.2 kornia-0.6.10 multidict-6.0.4 omegaconf-2.3.0 pyDeprecate-0.3.2 pytorch-lightning-1.6.5 taming-transformers-0.0.1 test-tube-0.7.5 tokenizers-0.13.2 torchmetrics-0.11.4 transformers-4.27.0 yarl-1.8.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 63.4.3\n",
            "    Uninstalling setuptools-63.4.3:\n",
            "      Successfully uninstalled setuptools-63.4.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 59.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-59.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pillow==9.0.1\n",
            "  Downloading Pillow-9.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 59.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pillow-9.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics==0.6.0\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.4/329.4 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics==0.6.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics==0.6.0) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics==0.6.0) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.1->torchmetrics==0.6.0) (4.5.0)\n",
            "Installing collected packages: torchmetrics\n",
            "  Attempting uninstall: torchmetrics\n",
            "    Found existing installation: torchmetrics 0.11.4\n",
            "    Uninstalling torchmetrics-0.11.4:\n",
            "      Successfully uninstalled torchmetrics-0.11.4\n",
            "Successfully installed torchmetrics-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp39-cp39-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m950.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp39-cp39-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.12.1\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp39-cp39-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.12.1+cu113) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision==0.13.1+cu113) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.13.1+cu113) (9.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision==0.13.1+cu113) (1.22.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.13.1+cu113) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.13.1+cu113) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.13.1+cu113) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.13.1+cu113) (1.26.15)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.1+cu116\n",
            "    Uninstalling torchaudio-0.13.1+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.13.1\n",
            "  Downloading torchtext-0.13.1-cp39-cp39-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext==0.13.1) (1.22.4)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.9/dist-packages (from torchtext==0.13.1) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext==0.13.1) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext==0.13.1) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.12.1->torchtext==0.13.1) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.13.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.13.1) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.13.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.13.1) (4.0.0)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.1\n",
            "    Uninstalling torchtext-0.14.1:\n",
            "      Successfully uninstalled torchtext-0.14.1\n",
            "Successfully installed torchtext-0.13.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/sd_text_inversion/stable-textual-inversion-cafe\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from latent-diffusion==0.0.1) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from latent-diffusion==0.0.1) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from latent-diffusion==0.0.1) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->latent-diffusion==0.0.1) (4.5.0)\n",
            "Installing collected packages: latent-diffusion\n",
            "  Running setup.py develop for latent-diffusion\n",
            "Successfully installed latent-diffusion-0.0.1\n",
            "Done. run the cell under this to enable all packages\n"
          ]
        }
      ],
      "source": [
        "#@title Installing the required\n",
        "#@markdown Please don't check this first, I haven't optimized it yet.\n",
        "use_drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "if use_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  %cd /content/drive/MyDrive\n",
        "else:\n",
        "  %cd /content\n",
        "\n",
        "!pip install -q -U gdown\n",
        "url = \"https://huggingface.co/andite/models/resolve/main/nai-wd.ckpt\" #@param {type:\"string\"}\n",
        "\n",
        "if use_drive:\n",
        "  !gdown {url} -O /content/drive/MyDrive/sd_text_inversion/\n",
        "else:\n",
        "  !gdown {url} -O /content/sd_text_inversion/\n",
        " \n",
        "if use_drive:\n",
        "  %cd /content/drive/MyDrive\n",
        "  !mkdir sd_text_inversion\n",
        "  %cd sd_text_inversion\n",
        "  !mkdir modelsbackup\n",
        "  !mkdir stable-textual-inversion-cafe\n",
        "  !mkdir logs\n",
        "  !mkdir output\n",
        "  !rm -rf stable-textual-inversion-cafe\n",
        "  %cd /content/drive/MyDrive/sd_text_inversion\n",
        "  !git clone https://github.com/Raearn/stable-textual-inversion-cafe.git\n",
        "  %cd stable-textual-inversion-cafe\n",
        "else:\n",
        "  %cd /content\n",
        "  !mkdir sd_text_inversion\n",
        "  %cd sd_text_inversion\n",
        "  !mkdir modelsbackup\n",
        "  !mkdir stable-textual-inversion-cafe\n",
        "  !mkdir logs\n",
        "  !mkdir output\n",
        "  !rm -rf stable-textual-inversion-cafe\n",
        "  %cd /content/sd_text_inversion\n",
        "  !git clone https://github.com/Raearn/stable-textual-inversion-cafe.git\n",
        "  %cd stable-textual-inversion-cafe\n",
        "\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "print(\"Done.\")\n",
        "\n",
        "if use_drive:\n",
        "  %cd /content/drive/MyDrive/sd_text_inversion/stable-textual-inversion-cafe\n",
        "else:\n",
        "  %cd /content/sd_text_inversion/stable-textual-inversion-cafe\n",
        "\n",
        "!pip install omegaconf einops pytorch-lightning==1.6.5 test-tube transformers kornia -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!pip install setuptools==59.5.0\n",
        "!pip install pillow==9.0.1\n",
        "!pip install torchmetrics==0.6.0\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install torchtext==0.13.1\n",
        "!pip install -e .\n",
        "\n",
        "print(\"Done. run the cell under this to start training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start TI Training\n",
        "#@markdown Dont't use any space in project_name, you can use underscore (_) instead.<p>\n",
        "auto = \"auto\"\n",
        "project_name = \"izu\" #@param {type:\"string\"}\n",
        "#@markdown If you change the `initializer_words`, make sure you keep the syntax correct.                       \n",
        "initializer_words = [\"girl\",\"face\",\"hair\",\"eyes\",\"headset\",\"clothes\"] #@param {type:\"raw\"}\n",
        "#@markdown If you are not sure how much vectors you need, you can set `num_vectors_per_token` to `auto` instead of a number. <p>\n",
        "num_vectors_per_token = auto #@param {type:\"raw\"}                                               \n",
        "ti_type = \"character\" #@param [\"character\", \"artstyle\"]\n",
        "model = \"/content/sd_text_inversion/nai-wd.ckpt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown # Dataset part\n",
        "#@markdown First please choose if you want to use a huggingface/drive link or a path <br>\n",
        "#@markdown Then specify the `url` or the `dataset_path` <br>\n",
        "\n",
        "path_type = \"path\" #@param [\"path\", \"url\"]\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "dataset_path = \"/content/train_data\" #@param {type:\"string\"}\n",
        "#@markdown ## *After setting each values, don't forget to click the run button.*\n",
        "#@title Downloading dataset and creating training configuration yaml\n",
        "#@markdown After running this cell, you can still check your configuration if you want to modify anything.\n",
        "#@markdown It is located in the `/content/sd_text_inversion/stable-textual-inversion-cafe/configs/stable-diffusion` folder, named as `<project_name>.yaml`\n",
        "\n",
        "import os\n",
        "\n",
        "# downloading dataset\n",
        "if (path_type == \"url\"):\n",
        "    if (url.startswith(\"https://drive.google.com\")):\n",
        "        !gdown {url} -O /content/{project_name}\n",
        "    else:\n",
        "        !gdown {url} -O /content/{project_name}.zip\n",
        "    !unzip /content/{project_name}.zip -d /content/sd_text_inversion/Imagesfortraining/{project_name}\n",
        "    num_images = len(os.listdir('/content/sd_text_inversion/Imagesfortraining/' + project_name))\n",
        "else:\n",
        "    num_images = len(os.listdir(dataset_path))\n",
        "\n",
        "# loading the base config\n",
        "import yaml\n",
        "with open(\"/content/sd_text_inversion/stable-textual-inversion-cafe/configs/stable-diffusion/\" + ti_type + \".yaml\", \"r\") as f:\n",
        "    base_config = yaml.safe_load(f)\n",
        "\n",
        "# calulate the number of vectors per token from the number of images\n",
        "if num_vectors_per_token == \"auto\":\n",
        "    num_vectors_per_token = round(num_images / 8.5)\n",
        "    if num_vectors_per_token < 8:\n",
        "        num_vectors_per_token = 8\n",
        "    if num_vectors_per_token > 18:\n",
        "        num_vectors_per_token = 18\n",
        "\n",
        "# calculating max steps\n",
        "if num_images <= 110:\n",
        "    max_steps = 11000\n",
        "else:\n",
        "    max_steps = 16000\n",
        "import math\n",
        "repeats = math.ceil(max_steps / num_images)\n",
        "\n",
        "# setting the parameters\n",
        "base_config[\"model\"][\"params\"][\"personalization_config\"][\"params\"][\"initializer_words\"] = initializer_words\n",
        "base_config[\"model\"][\"params\"][\"personalization_config\"][\"params\"][\"num_vectors_per_token\"] = num_vectors_per_token\n",
        "base_config[\"data\"][\"params\"][\"train\"][\"params\"][\"repeats\"] = repeats\n",
        "base_config[\"lightning\"][\"trainer\"][\"max_steps\"] = max_steps\n",
        "base_config[\"lightning\"][\"modelcheckpoint\"][\"params\"][\"every_n_train_steps\"] = 500\n",
        "base_config[\"lightning\"][\"callbacks\"][\"image_logger\"][\"params\"][\"batch_frequency\"] = 2000\n",
        "base_config[\"model\"][\"params\"][\"log_every_t\"] = 500\n",
        "\n",
        "# saving the config\n",
        "with open(\"/content/sd_text_inversion/stable-textual-inversion-cafe/configs/stable-diffusion/\" + project_name + \".yaml\", \"w\") as f:\n",
        "    yaml.dump(base_config, f)\n",
        "#@title Training\n",
        "#@markdown This block will train the TI, and when it's done, it will zip the checkpoints automatically. It will be available on your Google Drive, in `/content/sd_text_inversion/output` folder as `<project_name>.zip`\n",
        "if (ti_type == \"character\"):\n",
        "    initializer_word = \"character\"\n",
        "elif (ti_type == \"artstyle\"):\n",
        "    initializer_word = \"illustration\"\n",
        "\n",
        "if (path_type == \"url\"):\n",
        "  dataset = \"/content/sd_text_inversion/Imagesfortraining/\" + project_name\n",
        "else:\n",
        "  dataset = dataset_path\n",
        "config = \"/content/sd_text_inversion/stable-textual-inversion-cafe/configs/stable-diffusion/\" + project_name + \".yaml\"\n",
        "logs_folder = \"/content/sd_text_inversion/logs\"\n",
        "!mkdir /content/sd_text_inversion/logs\n",
        "!mkdir /content/sd_text_inversion/output\n",
        "\n",
        "%cd /content/sd_text_inversion/stable-textual-inversion-cafe\n",
        "!python \"main.py\" --base {config} -t --no-test --actual_resume {model}  -n {project_name} --gpus 1 --data_root {dataset} --init_word {initializer_word} --logdir {logs_folder}\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "dataset_name = dataset_path.split(\"/\")[-1]\n",
        "folders = os.listdir(logs_folder)\n",
        "folders = [folder for folder in folders if folder.startswith(dataset_name)]\n",
        "creation_times = [os.path.getctime(os.path.join(logs_folder, folder)) for folder in folders]\n",
        "ordered_folders = [folder for _, folder in sorted(zip(creation_times, folders))]\n",
        "newest_folder = ordered_folders[-1]\n",
        "\n",
        "%cd {logs_folder}/{newest_folder}\n",
        "!zip /content/sd_text_inversion/output/{project_name}.zip checkpoints/*\n",
        "print(\"Done! The checkpoints is saved in the output folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ_nxzI0xyE2",
        "outputId": "3aca0f71-ed0a-49d5-f478-5afe9a18a036"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/sd_text_inversion/logs’: File exists\n",
            "mkdir: cannot create directory ‘/content/sd_text_inversion/output’: File exists\n",
            "/content/sd_text_inversion/stable-textual-inversion-cafe\n",
            "Global seed set to 23\n",
            "Running on GPUs 1\n",
            "Loading model from /content/sd_text_inversion/nai-wd.ckpt\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Downloading (…)olve/main/vocab.json: 100% 961k/961k [00:00<00:00, 3.02MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 525k/525k [00:00<00:00, 2.09MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 389/389 [00:00<00:00, 171kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 905/905 [00:00<00:00, 359kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 4.52k/4.52k [00:00<00:00, 1.75MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.71G/1.71G [00:07<00:00, 238MB/s]\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.15.layer_norm2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Restored from /content/sd_text_inversion/nai-wd.ckpt with 0 missing and 3 unexpected keys\n",
            "Unexpected Keys: ['state_dict', 'model_ema.decay', 'model_ema.num_updates']\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loggers/test_tube.py:105: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.\n",
            "  rank_zero_deprecation(\n",
            "Monitoring val/loss_simple_ema as checkpoint metric.\n",
            "Merged modelckpt-cfg: \n",
            "{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': '/content/sd_text_inversion/logs/train_data2023-03-15T19-50-17_izu/checkpoints', 'filename': 'gs-{global_step:06}_e-{epoch:06}', 'every_n_train_steps': 500, 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 1}}\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:297: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "#### Data #####\n",
            "train, PersonalizedBase, 11020\n",
            "validation, PersonalizedBase, 29\n",
            "accumulate_grad_batches = 1\n",
            "Setting learning rate to 5.00e-03 = 1 (accumulate_grad_batches) * 1 (num_gpus) * 1 (batchsize) * 5.00e-03 (base_lr)\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/configuration_validator.py:326: LightningDeprecationWarning: Base `LightningModule.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
            "  rank_zero_deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
            "  rank_zero_deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
            "  rank_zero_deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/configuration_validator.py:342: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
            "  rank_zero_deprecation(\n",
            "Global seed set to 23\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/distributed.py:316: LightningDeprecationWarning: Environment variable `PL_TORCH_DISTRIBUTED_BACKEND` was deprecated in v1.6 and will be removed in v1.8. Specify `process_group_backend` directly on the strategy constructor.\n",
            "  rank_zero_deprecation(\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=gloo\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name              | Type               | Params\n",
            "---------------------------------------------------------\n",
            "0 | model             | DiffusionWrapper   | 859 M \n",
            "1 | first_stage_model | AutoencoderKL      | 83.7 M\n",
            "2 | cond_stage_model  | FrozenCLIPEmbedder | 123 M \n",
            "3 | embedding_manager | EmbeddingManager   | 12.3 K\n",
            "---------------------------------------------------------\n",
            "6.1 K     Trainable params\n",
            "1.1 B     Non-trainable params\n",
            "1.1 B     Total params\n",
            "4,264.990 Total estimated model params size (MB)\n",
            "2023-03-15 19:51:32.563562: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-15 19:51:36.598028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-15 19:51:36.598366: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-15 19:51:36.598389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Project config\n",
            "data:\n",
            "  params:\n",
            "    batch_size: 1\n",
            "    num_workers: 16\n",
            "    train:\n",
            "      params:\n",
            "        per_image_tokens: false\n",
            "        repeats: 380\n",
            "        set: train\n",
            "        size: 512\n",
            "        templates:\n",
            "        - a photo of a {}\n",
            "        - a rendering of a {}\n",
            "        - a cropped photo of the {}\n",
            "        - the photo of a {}\n",
            "        - a photo of a clean {}\n",
            "        - a photo of a dirty {}\n",
            "        - a dark photo of the {}\n",
            "        - a photo of my {}\n",
            "        - a photo of the cool {}\n",
            "        - a close-up photo of a {}\n",
            "        - a bright photo of the {}\n",
            "        - a cropped photo of a {}\n",
            "        - a photo of the {}\n",
            "        - a good photo of the {}\n",
            "        - a photo of one {}\n",
            "        - a close-up photo of the {}\n",
            "        - a rendition of the {}\n",
            "        - a photo of the clean {}\n",
            "        - a rendition of a {}\n",
            "        - a photo of a nice {}\n",
            "        - a good photo of a {}\n",
            "        - a photo of the nice {}\n",
            "        - a photo of the small {}\n",
            "        - a photo of the weird {}\n",
            "        - a photo of the large {}\n",
            "        - a photo of a cool {}\n",
            "        - a photo of a small {}\n",
            "      target: ldm.data.personalized.PersonalizedBase\n",
            "    validation:\n",
            "      params:\n",
            "        per_image_tokens: false\n",
            "        repeats: 10\n",
            "        set: val\n",
            "        size: 512\n",
            "        templates:\n",
            "        - a photo of a {}\n",
            "        - a rendering of a {}\n",
            "        - a cropped photo of the {}\n",
            "        - the photo of a {}\n",
            "        - a photo of a clean {}\n",
            "        - a photo of a dirty {}\n",
            "        - a dark photo of the {}\n",
            "        - a photo of my {}\n",
            "        - a photo of the cool {}\n",
            "        - a close-up photo of a {}\n",
            "        - a bright photo of the {}\n",
            "        - a cropped photo of a {}\n",
            "        - a photo of the {}\n",
            "        - a good photo of the {}\n",
            "        - a photo of one {}\n",
            "        - a close-up photo of the {}\n",
            "        - a rendition of the {}\n",
            "        - a photo of the clean {}\n",
            "        - a rendition of a {}\n",
            "        - a photo of a nice {}\n",
            "        - a good photo of a {}\n",
            "        - a photo of the nice {}\n",
            "        - a photo of the small {}\n",
            "        - a photo of the weird {}\n",
            "        - a photo of the large {}\n",
            "        - a photo of a cool {}\n",
            "        - a photo of a small {}\n",
            "      target: ldm.data.personalized.PersonalizedBase\n",
            "    wrap: false\n",
            "  target: main.DataModuleFromConfig\n",
            "model:\n",
            "  base_learning_rate: 0.005\n",
            "  params:\n",
            "    channels: 4\n",
            "    cond_stage_config:\n",
            "      params:\n",
            "        extended_mode: true\n",
            "        max_chunks: 3\n",
            "        penultimate: true\n",
            "      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder\n",
            "    cond_stage_key: caption\n",
            "    cond_stage_trainable: true\n",
            "    conditioning_key: crossattn\n",
            "    embedding_reg_weight: 0.0\n",
            "    first_stage_config:\n",
            "      params:\n",
            "        ddconfig:\n",
            "          attn_resolutions: []\n",
            "          ch: 128\n",
            "          ch_mult:\n",
            "          - 1\n",
            "          - 2\n",
            "          - 4\n",
            "          - 4\n",
            "          double_z: true\n",
            "          dropout: 0.0\n",
            "          in_channels: 3\n",
            "          num_res_blocks: 2\n",
            "          out_ch: 3\n",
            "          resolution: 512\n",
            "          z_channels: 4\n",
            "        embed_dim: 4\n",
            "        lossconfig:\n",
            "          target: torch.nn.Identity\n",
            "        monitor: val/rec_loss\n",
            "      target: ldm.models.autoencoder.AutoencoderKL\n",
            "    first_stage_key: image\n",
            "    image_size: 64\n",
            "    linear_end: 0.012\n",
            "    linear_start: 0.00085\n",
            "    log_every_t: 500\n",
            "    monitor: val/loss_simple_ema\n",
            "    num_timesteps_cond: 1\n",
            "    personalization_config:\n",
            "      params:\n",
            "        initializer_words:\n",
            "        - character\n",
            "        - face\n",
            "        - hair\n",
            "        - eyes\n",
            "        - headset\n",
            "        - clothes\n",
            "        num_vectors_per_token: 8\n",
            "        per_image_tokens: false\n",
            "        placeholder_strings:\n",
            "        - '*'\n",
            "        progressive_words: false\n",
            "        embedding_manager_ckpt: ''\n",
            "        placeholder_tokens:\n",
            "        - '*'\n",
            "      target: ldm.modules.embedding_manager.EmbeddingManager\n",
            "    scale_factor: 0.18215\n",
            "    timesteps: 1000\n",
            "    unet_config:\n",
            "      params:\n",
            "        attention_resolutions:\n",
            "        - 4\n",
            "        - 2\n",
            "        - 1\n",
            "        channel_mult:\n",
            "        - 1\n",
            "        - 2\n",
            "        - 4\n",
            "        - 4\n",
            "        context_dim: 768\n",
            "        image_size: 32\n",
            "        in_channels: 4\n",
            "        legacy: false\n",
            "        model_channels: 320\n",
            "        num_heads: 8\n",
            "        num_res_blocks: 2\n",
            "        out_channels: 4\n",
            "        transformer_depth: 1\n",
            "        use_checkpoint: true\n",
            "        use_spatial_transformer: true\n",
            "      target: ldm.modules.diffusionmodules.openaimodel.UNetModel\n",
            "    use_ema: false\n",
            "    ckpt_path: /content/sd_text_inversion/nai-wd.ckpt\n",
            "  target: ldm.models.diffusion.ddpm.LatentDiffusion\n",
            "\n",
            "Lightning config\n",
            "callbacks:\n",
            "  image_logger:\n",
            "    params:\n",
            "      batch_frequency: 2000\n",
            "      increase_log_steps: false\n",
            "      max_images: 8\n",
            "    target: main.ImageLogger\n",
            "modelcheckpoint:\n",
            "  params:\n",
            "    every_n_train_steps: 500\n",
            "trainer:\n",
            "  benchmark: true\n",
            "  find_unused_parameters: false\n",
            "  max_steps: 11000\n",
            "  accelerator: ddp\n",
            "  gpus: 1\n",
            "\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Training: 0it [00:00, ?it/s]/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
            "  rank_zero_deprecation(\n",
            "Epoch 0:   0% 0/11049 [00:00<?, ?it/s] /usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "  warning_cache.warn(\n",
            "[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "Epoch 0:   5% 500/11049 [08:07<2:51:16,  1.03it/s, loss=0.107, v_num=0, train/loss_simple_step=0.127, train/loss_vlb_step=0.000517, train/loss_step=0.127, global_step=499.0]  /usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:378: UserWarning: `ModelCheckpoint(monitor='val/loss_simple_ema')` could not find the monitored key in the returned metrics: ['train/loss_simple', 'train/loss_simple_step', 'train/loss_vlb', 'train/loss_vlb_step', 'train/loss', 'train/loss_step', 'global_step', 'epoch', 'step']. HINT: Did you call `log('val/loss_simple_ema', value)` in the `LightningModule`?\n",
            "  warning_cache.warn(m)\n",
            "Epoch 0, global step 500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:   9% 1000/11049 [16:10<2:42:30,  1.03it/s, loss=0.058, v_num=0, train/loss_simple_step=0.0899, train/loss_vlb_step=0.000298, train/loss_step=0.0899, global_step=999.0]Epoch 0, global step 1000: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  14% 1500/11049 [24:12<2:34:08,  1.03it/s, loss=0.0962, v_num=0, train/loss_simple_step=0.031, train/loss_vlb_step=0.000121, train/loss_step=0.031, global_step=1499.0]   Epoch 0, global step 1500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  18% 1980/11049 [31:56<2:26:16,  1.03it/s, loss=0.121, v_num=0, train/loss_simple_step=0.0119, train/loss_vlb_step=5.62e-5, train/loss_step=0.0119, global_step=1979.0]Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:00<00:15,  3.17it/s]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:00<00:15,  3.17it/s]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:00<00:15,  3.03it/s]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:01<00:14,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:01<00:14,  3.13it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:01<00:14,  3.06it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:02<00:13,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:02<00:13,  3.12it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:02<00:13,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:03<00:12,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:03<00:12,  3.11it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:03<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:04<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:04<00:11,  3.11it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:05<00:11,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:05<00:10,  3.11it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:05<00:10,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:06<00:10,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:06<00:09,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:06<00:09,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:07<00:09,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:07<00:08,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:07<00:08,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:08<00:08,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:08<00:07,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:08<00:07,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:09<00:07,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:09<00:06,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:09<00:06,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:10<00:06,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:10<00:05,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:10<00:05,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:11<00:05,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:11<00:04,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:11<00:04,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:11<00:04,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:12<00:03,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:12<00:03,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:12<00:03,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:13<00:02,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:13<00:02,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:13<00:02,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:14<00:01,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:14<00:01,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:14<00:01,  3.06it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:15<00:00,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:15<00:00,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:15<00:00,  3.06it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:16<00:00,  3.08it/s]\n",
            "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:04<03:27,  4.23s/it]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:04<01:40,  2.10s/it]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:05<01:07,  1.43s/it]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:06<00:51,  1.12s/it]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:06<00:42,  1.07it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:07<00:36,  1.20it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:07<00:32,  1.31it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:08<00:30,  1.38it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:09<00:28,  1.44it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:09<00:26,  1.49it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:10<00:25,  1.52it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:11<00:24,  1.54it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:11<00:23,  1.55it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:12<00:23,  1.56it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:13<00:22,  1.57it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:13<00:21,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:14<00:20,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:14<00:20,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:15<00:19,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:16<00:18,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:16<00:18,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:17<00:17,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:18<00:16,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:18<00:16,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:19<00:15,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:19<00:15,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:20<00:14,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:21<00:13,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:21<00:13,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:22<00:12,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:23<00:11,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:23<00:11,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:24<00:10,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:24<00:10,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:25<00:09,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:26<00:08,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:26<00:08,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:27<00:07,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:28<00:06,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:28<00:06,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:29<00:05,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:29<00:05,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:30<00:04,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:31<00:03,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:31<00:03,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:32<00:02,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:33<00:01,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:33<00:01,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:34<00:00,  1.58it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:35<00:00,  1.43it/s]\n",
            "Epoch 0:  18% 2000/11049 [33:13<2:30:19,  1.00it/s, loss=0.0795, v_num=0, train/loss_simple_step=0.0991, train/loss_vlb_step=0.000326, train/loss_step=0.0991, global_step=2e+3]Epoch 0, global step 2000: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  23% 2500/11049 [41:16<2:21:10,  1.01it/s, loss=0.0921, v_num=0, train/loss_simple_step=0.00648, train/loss_vlb_step=3.03e-5, train/loss_step=0.00648, global_step=2499.0]Epoch 0, global step 2500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  27% 3000/11049 [49:20<2:12:23,  1.01it/s, loss=0.121, v_num=0, train/loss_simple_step=0.184, train/loss_vlb_step=0.000627, train/loss_step=0.184, global_step=3e+3]  Epoch 0, global step 3000: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  32% 3500/11049 [57:23<2:03:47,  1.02it/s, loss=0.115, v_num=0, train/loss_simple_step=0.183, train/loss_vlb_step=0.000899, train/loss_step=0.183, global_step=3499.0]  Epoch 0, global step 3500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  36% 3980/11049 [1:05:07<1:55:40,  1.02it/s, loss=0.0843, v_num=0, train/loss_simple_step=0.0125, train/loss_vlb_step=5.56e-5, train/loss_step=0.0125, global_step=3979.0]pop from empty list\n",
            "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:00<00:16,  3.05it/s]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:00<00:15,  3.12it/s]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:00<00:15,  3.13it/s]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:01<00:14,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:01<00:14,  3.11it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:01<00:14,  3.11it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:02<00:14,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:02<00:13,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:02<00:13,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:03<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:03<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:03<00:12,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:04<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:04<00:11,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:05<00:11,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:05<00:10,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:05<00:10,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:06<00:10,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:06<00:09,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:06<00:09,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:07<00:09,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:07<00:08,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:07<00:08,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:08<00:08,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:08<00:07,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:08<00:07,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:09<00:07,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:09<00:06,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:09<00:06,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:10<00:06,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:10<00:05,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:10<00:05,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:11<00:05,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:11<00:04,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:11<00:04,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:11<00:04,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:12<00:03,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:12<00:03,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:12<00:03,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:13<00:02,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:13<00:02,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:13<00:02,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:14<00:01,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:14<00:01,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:14<00:01,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:15<00:00,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:15<00:00,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:15<00:00,  3.09it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:16<00:00,  3.08it/s]\n",
            "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:00<00:30,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:01<00:30,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:01<00:29,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:02<00:28,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:03<00:28,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:03<00:27,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:04<00:26,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:05<00:26,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:05<00:25,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:06<00:25,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:06<00:24,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:07<00:23,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:08<00:23,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:08<00:22,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:09<00:21,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:10<00:21,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:10<00:20,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:11<00:20,  1.60it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:11<00:19,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:12<00:18,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:13<00:18,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:13<00:17,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:14<00:16,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:15<00:16,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:15<00:15,  1.60it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:16<00:15,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:16<00:14,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:17<00:13,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:18<00:13,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:18<00:12,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:19<00:11,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:20<00:11,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:20<00:10,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:21<00:10,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:21<00:09,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:22<00:08,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:23<00:08,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:23<00:07,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:24<00:06,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:25<00:06,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:25<00:05,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:26<00:05,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:27<00:04,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:27<00:03,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:28<00:03,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:28<00:02,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:29<00:01,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:30<00:01,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:30<00:00,  1.59it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:31<00:00,  1.59it/s]\n",
            "Epoch 0:  36% 4000/11049 [1:06:17<1:56:49,  1.01it/s, loss=0.124, v_num=0, train/loss_simple_step=0.0455, train/loss_vlb_step=0.000167, train/loss_step=0.0455, global_step=4e+3]  Epoch 0, global step 4000: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  41% 4500/11049 [1:14:20<1:48:11,  1.01it/s, loss=0.0933, v_num=0, train/loss_simple_step=0.00695, train/loss_vlb_step=3.52e-5, train/loss_step=0.00695, global_step=4499.0]Epoch 0, global step 4500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  45% 5000/11049 [1:22:23<1:39:40,  1.01it/s, loss=0.0678, v_num=0, train/loss_simple_step=0.0045, train/loss_vlb_step=2.51e-5, train/loss_step=0.0045, global_step=5e+3]  Epoch 0, global step 5000: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  50% 5500/11049 [1:30:26<1:31:14,  1.01it/s, loss=0.11, v_num=0, train/loss_simple_step=0.208, train/loss_vlb_step=0.000734, train/loss_step=0.208, global_step=5499.0]  Epoch 0, global step 5500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  54% 5980/11049 [1:38:10<1:23:12,  1.02it/s, loss=0.0759, v_num=0, train/loss_simple_step=0.0359, train/loss_vlb_step=0.000134, train/loss_step=0.0359, global_step=5979.0]pop from empty list\n",
            "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:00<00:16,  3.02it/s]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:00<00:15,  3.11it/s]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:00<00:15,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:01<00:15,  3.05it/s]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:01<00:14,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:01<00:14,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:02<00:14,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:02<00:13,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:02<00:13,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:03<00:13,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:03<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:03<00:12,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:04<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:04<00:11,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:05<00:11,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:05<00:10,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:05<00:10,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:06<00:10,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:06<00:09,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:06<00:09,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:07<00:09,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:07<00:08,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:07<00:08,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:08<00:08,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:08<00:07,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:08<00:07,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:09<00:07,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:09<00:06,  3.05it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:09<00:06,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:10<00:06,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:10<00:05,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:10<00:05,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:11<00:05,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:11<00:04,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:11<00:04,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:12<00:04,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:12<00:03,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:12<00:03,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:12<00:03,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:13<00:02,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:13<00:02,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:13<00:02,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:14<00:01,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:14<00:01,  3.06it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:14<00:01,  3.06it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:15<00:00,  3.06it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:15<00:00,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:15<00:00,  3.08it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:16<00:00,  3.08it/s]\n",
            "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:00<00:30,  1.60it/s]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:01<00:30,  1.60it/s]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:01<00:29,  1.60it/s]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:02<00:28,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:03<00:28,  1.60it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:03<00:27,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:04<00:26,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:05<00:26,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:05<00:25,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:06<00:25,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:06<00:24,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:07<00:23,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:08<00:23,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:08<00:22,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:09<00:21,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:10<00:21,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:10<00:20,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:11<00:20,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:11<00:19,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:12<00:18,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:13<00:18,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:13<00:17,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:14<00:16,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:15<00:16,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:15<00:15,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:16<00:15,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:16<00:14,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:17<00:13,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:18<00:13,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:18<00:12,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:19<00:12,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:20<00:11,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:20<00:10,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:21<00:10,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:22<00:09,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:22<00:08,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:23<00:08,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:23<00:07,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:24<00:06,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:25<00:06,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:25<00:05,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:26<00:05,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:27<00:04,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:27<00:03,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:28<00:03,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:28<00:02,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:29<00:01,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:30<00:01,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:30<00:00,  1.59it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:31<00:00,  1.59it/s]\n",
            "Epoch 0:  54% 6000/11049 [1:39:20<1:23:35,  1.01it/s, loss=0.065, v_num=0, train/loss_simple_step=0.0296, train/loss_vlb_step=0.000112, train/loss_step=0.0296, global_step=6e+3]   Epoch 0, global step 6000: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  59% 6500/11049 [1:47:24<1:15:09,  1.01it/s, loss=0.112, v_num=0, train/loss_simple_step=0.199, train/loss_vlb_step=0.000787, train/loss_step=0.199, global_step=6499.0]  Epoch 0, global step 6500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  63% 7000/11049 [1:55:27<1:06:46,  1.01it/s, loss=0.115, v_num=0, train/loss_simple_step=0.0337, train/loss_vlb_step=0.000132, train/loss_step=0.0337, global_step=7e+3]Epoch 0, global step 7000: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  68% 7500/11049 [2:03:29<58:26,  1.01it/s, loss=0.0917, v_num=0, train/loss_simple_step=0.102, train/loss_vlb_step=0.000356, train/loss_step=0.102, global_step=7499.0]  Epoch 0, global step 7500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  72% 7980/11049 [2:11:13<50:27,  1.01it/s, loss=0.0556, v_num=0, train/loss_simple_step=0.0579, train/loss_vlb_step=0.000192, train/loss_step=0.0579, global_step=7979.0]pop from empty list\n",
            "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:00<00:15,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:00<00:15,  3.12it/s]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:00<00:15,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:01<00:14,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:01<00:14,  3.11it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:01<00:14,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:02<00:13,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:02<00:13,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:02<00:13,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:03<00:13,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:03<00:12,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:03<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:05<00:11,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:05<00:10,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:05<00:10,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:06<00:10,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:06<00:09,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:06<00:09,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:07<00:09,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:07<00:08,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:07<00:08,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:08<00:08,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:08<00:07,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:08<00:07,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:09<00:07,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:09<00:06,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:09<00:06,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:10<00:06,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:10<00:05,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:10<00:05,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:11<00:05,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:11<00:04,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:11<00:04,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:11<00:04,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:12<00:03,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:12<00:03,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:12<00:03,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:13<00:02,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:13<00:02,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:13<00:02,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:14<00:01,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:14<00:01,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:14<00:01,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:15<00:00,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:15<00:00,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:15<00:00,  3.09it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:16<00:00,  3.09it/s]\n",
            "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:00<00:30,  1.61it/s]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:01<00:30,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:01<00:29,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:02<00:28,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:03<00:28,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:03<00:27,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:04<00:27,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:05<00:26,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:05<00:25,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:06<00:25,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:06<00:24,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:07<00:23,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:08<00:23,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:08<00:22,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:09<00:21,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:10<00:21,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:10<00:20,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:11<00:20,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:11<00:19,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:12<00:18,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:13<00:18,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:13<00:17,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:14<00:16,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:15<00:16,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:15<00:15,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:16<00:15,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:16<00:14,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:17<00:13,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:18<00:13,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:18<00:12,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:19<00:11,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:20<00:11,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:20<00:10,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:21<00:10,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:21<00:09,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:22<00:08,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:23<00:08,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:23<00:07,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:24<00:06,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:25<00:06,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:25<00:05,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:26<00:05,  1.60it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:27<00:04,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:27<00:03,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:28<00:03,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:28<00:02,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:29<00:01,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:30<00:01,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:30<00:00,  1.59it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:31<00:00,  1.59it/s]\n",
            "Epoch 0:  72% 8000/11049 [2:12:23<50:27,  1.01it/s, loss=0.0546, v_num=0, train/loss_simple_step=0.128, train/loss_vlb_step=0.000443, train/loss_step=0.128, global_step=8e+3]    Epoch 0, global step 8000: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  77% 8500/11049 [2:20:26<42:06,  1.01it/s, loss=0.092, v_num=0, train/loss_simple_step=0.064, train/loss_vlb_step=0.000229, train/loss_step=0.064, global_step=8499.0]   Epoch 0, global step 8500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  81% 9000/11049 [2:28:28<33:48,  1.01it/s, loss=0.0842, v_num=0, train/loss_simple_step=0.0877, train/loss_vlb_step=0.000293, train/loss_step=0.0877, global_step=9e+3]Epoch 0, global step 9000: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  86% 9500/11049 [2:36:32<25:31,  1.01it/s, loss=0.104, v_num=0, train/loss_simple_step=0.00278, train/loss_vlb_step=1.65e-5, train/loss_step=0.00278, global_step=9499.0]Epoch 0, global step 9500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  90% 9980/11049 [2:44:15<17:35,  1.01it/s, loss=0.16, v_num=0, train/loss_simple_step=0.102, train/loss_vlb_step=0.00034, train/loss_step=0.102, global_step=9979.0]    pop from empty list\n",
            "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:00<00:15,  3.12it/s]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:00<00:15,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:00<00:15,  3.11it/s]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:01<00:14,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:01<00:14,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:01<00:14,  3.11it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:02<00:13,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:02<00:13,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:02<00:13,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:03<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:03<00:12,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:03<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:04<00:12,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:04<00:11,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:05<00:11,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:05<00:10,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:05<00:10,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:06<00:10,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:06<00:09,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:06<00:09,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:07<00:09,  3.10it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:07<00:08,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:07<00:08,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:08<00:08,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:08<00:07,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:08<00:07,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:09<00:07,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:09<00:06,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:09<00:06,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:10<00:06,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:10<00:05,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:10<00:05,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:11<00:05,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:11<00:04,  3.07it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:11<00:04,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:11<00:04,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:12<00:03,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:12<00:03,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:12<00:03,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:13<00:02,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:13<00:02,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:13<00:02,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:14<00:01,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:14<00:01,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:14<00:01,  3.08it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:15<00:00,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:15<00:00,  3.09it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:15<00:00,  3.09it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:16<00:00,  3.08it/s]\n",
            "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 50 timesteps\n",
            "\n",
            "DDIM Sampler:   0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "DDIM Sampler:   2% 1/50 [00:00<00:30,  1.62it/s]\u001b[A\n",
            "DDIM Sampler:   4% 2/50 [00:01<00:30,  1.60it/s]\u001b[A\n",
            "DDIM Sampler:   6% 3/50 [00:01<00:29,  1.60it/s]\u001b[A\n",
            "DDIM Sampler:   8% 4/50 [00:02<00:28,  1.60it/s]\u001b[A\n",
            "DDIM Sampler:  10% 5/50 [00:03<00:28,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  12% 6/50 [00:03<00:27,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  14% 7/50 [00:04<00:26,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  16% 8/50 [00:05<00:26,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  18% 9/50 [00:05<00:25,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  20% 10/50 [00:06<00:25,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  22% 11/50 [00:06<00:24,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  24% 12/50 [00:07<00:23,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  26% 13/50 [00:08<00:23,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  28% 14/50 [00:08<00:22,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  30% 15/50 [00:09<00:21,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  32% 16/50 [00:10<00:21,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  34% 17/50 [00:10<00:20,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  36% 18/50 [00:11<00:20,  1.59it/s]\u001b[A\n",
            "DDIM Sampler:  38% 19/50 [00:11<00:19,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  40% 20/50 [00:12<00:19,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  42% 21/50 [00:13<00:18,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  44% 22/50 [00:13<00:17,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  46% 23/50 [00:14<00:17,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  48% 24/50 [00:15<00:16,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  50% 25/50 [00:15<00:15,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  52% 26/50 [00:16<00:15,  1.57it/s]\u001b[A\n",
            "DDIM Sampler:  54% 27/50 [00:17<00:14,  1.57it/s]\u001b[A\n",
            "DDIM Sampler:  56% 28/50 [00:17<00:13,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  58% 29/50 [00:18<00:13,  1.57it/s]\u001b[A\n",
            "DDIM Sampler:  60% 30/50 [00:18<00:12,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  62% 31/50 [00:19<00:12,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  64% 32/50 [00:20<00:11,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  66% 33/50 [00:20<00:10,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  68% 34/50 [00:21<00:10,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  70% 35/50 [00:22<00:09,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  72% 36/50 [00:22<00:08,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  74% 37/50 [00:23<00:08,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  76% 38/50 [00:23<00:07,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  78% 39/50 [00:24<00:06,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  80% 40/50 [00:25<00:06,  1.57it/s]\u001b[A\n",
            "DDIM Sampler:  82% 41/50 [00:25<00:05,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  84% 42/50 [00:26<00:05,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  86% 43/50 [00:27<00:04,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  88% 44/50 [00:27<00:03,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  90% 45/50 [00:28<00:03,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  92% 46/50 [00:29<00:02,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  94% 47/50 [00:29<00:01,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  96% 48/50 [00:30<00:01,  1.58it/s]\u001b[A\n",
            "DDIM Sampler:  98% 49/50 [00:30<00:00,  1.58it/s]\u001b[A\n",
            "DDIM Sampler: 100% 50/50 [00:31<00:00,  1.58it/s]\n",
            "Epoch 0:  91% 10000/11049 [2:45:25<17:21,  1.01it/s, loss=0.0753, v_num=0, train/loss_simple_step=0.00592, train/loss_vlb_step=3.25e-5, train/loss_step=0.00592, global_step=1e+4]Epoch 0, global step 10000: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0:  95% 10500/11049 [2:53:28<09:04,  1.01it/s, loss=0.0848, v_num=0, train/loss_simple_step=0.131, train/loss_vlb_step=0.000476, train/loss_step=0.131, global_step=10499.0]Epoch 0, global step 10500: 'val/loss_simple_ema' was not in top 1\n",
            "Epoch 0: 100% 11000/11049 [3:01:31<00:48,  1.01it/s, loss=0.105, v_num=0, train/loss_simple_step=0.102, train/loss_vlb_step=0.000389, train/loss_step=0.102, global_step=1.1e+4]   Epoch 0, global step 11000: 'val/loss_simple_ema' was not in top 1\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
            "  rank_zero_deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:2028: LightningDeprecationWarning: `Trainer.training_type_plugin` is deprecated in v1.6 and will be removed in v1.8. Use `Trainer.strategy` instead.\n",
            "  rank_zero_deprecation(\n",
            "Average Epoch time: 10891.38 seconds\n",
            "Average Peak memory 11484.31MiB\n",
            "Epoch 0: 100% 11000/11049 [3:01:31<00:48,  1.01it/s, loss=0.105, v_num=0, train/loss_simple_step=0.102, train/loss_vlb_step=0.000389, train/loss_step=0.102, global_step=1.1e+4, train/loss_simple_epoch=0.093, train/loss_vlb_epoch=0.00164, train/loss_epoch=0.093]\n",
            "^C\n",
            "/content/sd_text_inversion/logs/train_data2023-03-15T19-50-17_izu\n",
            "  adding: checkpoints/embeddings_gs-10000.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-1000.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-10500.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-11000.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-1500.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-2000.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-2500.pt (deflated 12%)\n",
            "  adding: checkpoints/embeddings_gs-3000.pt (deflated 12%)\n",
            "  adding: checkpoints/embeddings_gs-3500.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-4000.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-4500.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-5000.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-500.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-5500.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-6000.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-6500.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-7000.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-7500.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-8000.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-8500.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-9000.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings_gs-9500.pt (deflated 11%)\n",
            "  adding: checkpoints/embeddings.pt (deflated 11%)\n",
            "  adding: checkpoints/last.ckpt (deflated 63%)\n",
            "Done! The checkpoints is saved in the output folder.\n"
          ]
        }
      ]
    }
  ]
}