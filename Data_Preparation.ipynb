{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KitsunekoFi/ArielACE/blob/main/Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slgjeYgd6pWp"
      },
      "source": [
        "# **Reserval Data Preparation**\n",
        "A Colab Notebook For SDXL LoRA Training (Fine-tuning Method)\n",
        "\n",
        "[visitor-badge]: https://api.visitorbadge.io/api/visitors?path=Kohya%20LoRA%20Trainer%20XL&label=Visitors&labelColor=%2334495E&countColor=%231ABC9C&style=flat&labelStyle=none\n",
        "[visitor-stats]: https://visitorbadge.io/status?path=Kohya%20LoRA%20Trainer%20XL\n",
        "[ko-fi-badge]: https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat\n",
        "[ko-fi-link]: https://ko-fi.com/linaqruf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_u3q60di584x"
      },
      "outputs": [],
      "source": [
        "# @title ## **1.1. Install Kohya Trainer**\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "import requests\n",
        "import torch\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "from google.colab import drive\n",
        "\n",
        "%store -r\n",
        "\n",
        "# root_dir\n",
        "root_dir          = \"/content\"\n",
        "drive_dir         = os.path.join(root_dir, \"drive/MyDrive\")\n",
        "deps_dir          = os.path.join(root_dir, \"deps\")\n",
        "repo_dir          = os.path.join(root_dir, \"kohya-trainer\")\n",
        "training_dir      = os.path.join(root_dir, \"LoRA\")\n",
        "pretrained_model  = os.path.join(root_dir, \"pretrained_model\")\n",
        "vae_dir           = os.path.join(root_dir, \"vae\")\n",
        "lora_dir          = os.path.join(root_dir, \"network_weight\")\n",
        "repositories_dir  = os.path.join(root_dir, \"repositories\")\n",
        "config_dir        = os.path.join(training_dir, \"config\")\n",
        "tools_dir         = os.path.join(repo_dir, \"tools\")\n",
        "finetune_dir      = os.path.join(repo_dir, \"finetune\")\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "for store in [\"root_dir\", \"repo_dir\", \"training_dir\", \"pretrained_model\", \"vae_dir\", \"repositories_dir\", \"accelerate_config\", \"tools_dir\", \"finetune_dir\", \"config_dir\"]:\n",
        "    with capture.capture_output() as cap:\n",
        "        %store {store}\n",
        "        del cap\n",
        "\n",
        "repo_dict = {\n",
        "    \"qaneel/kohya-trainer (forked repo, stable, optimized for colab use)\" : \"https://github.com/qaneel/kohya-trainer\",\n",
        "    \"kohya-ss/sd-scripts (original repo, latest update)\"                    : \"https://github.com/kohya-ss/sd-scripts\",\n",
        "}\n",
        "\n",
        "repository        = \"qaneel/kohya-trainer (forked repo, stable, optimized for colab use)\" #@param [\"qaneel/kohya-trainer (forked repo, stable, optimized for colab use)\", \"kohya-ss/sd-scripts (original repo, latest update)\"] {allow-input: true}\n",
        "repo_url          = repo_dict[repository]\n",
        "branch            = \"main\"  # @param {type: \"string\"}\n",
        "output_to_drive   = False  # @param {type: \"boolean\"}\n",
        "\n",
        "def clone_repo(url, dir, branch):\n",
        "    if not os.path.exists(dir):\n",
        "       !git clone -b {branch} {url} {dir}\n",
        "\n",
        "def mount_drive(dir):\n",
        "    output_dir      = os.path.join(training_dir, \"output\")\n",
        "\n",
        "    if output_to_drive:\n",
        "        if not os.path.exists(drive_dir):\n",
        "            drive.mount(os.path.dirname(drive_dir))\n",
        "        output_dir  = os.path.join(drive_dir, \"kohya-trainer/output\")\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "def setup_directories():\n",
        "    global output_dir\n",
        "\n",
        "    output_dir      = mount_drive(drive_dir)\n",
        "\n",
        "    for dir in [training_dir, config_dir, pretrained_model, vae_dir, repositories_dir, output_dir]:\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "def pastebin_reader(id):\n",
        "    if \"pastebin.com\" in id:\n",
        "        url = id\n",
        "        if 'raw' not in url:\n",
        "                url = url.replace('pastebin.com', 'pastebin.com/raw')\n",
        "    else:\n",
        "        url = \"https://pastebin.com/raw/\" + id\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    lines = response.text.split('\\n')\n",
        "    return lines\n",
        "\n",
        "def install_repository():\n",
        "    global infinite_image_browser_dir, voldy, discordia_archivum_dir\n",
        "\n",
        "    _, voldy = pastebin_reader(\"kq6ZmHFU\")[:2]\n",
        "\n",
        "    infinite_image_browser_url  = f\"https://github.com/zanllp/{voldy}-infinite-image-browsing.git\"\n",
        "    infinite_image_browser_dir  = os.path.join(repositories_dir, f\"infinite-image-browsing\")\n",
        "    infinite_image_browser_deps = os.path.join(infinite_image_browser_dir, \"requirements.txt\")\n",
        "\n",
        "    discordia_archivum_url = \"https://github.com/Linaqruf/discordia-archivum\"\n",
        "    discordia_archivum_dir = os.path.join(repositories_dir, \"discordia-archivum\")\n",
        "    discordia_archivum_deps = os.path.join(discordia_archivum_dir, \"requirements.txt\")\n",
        "\n",
        "    clone_repo(infinite_image_browser_url, infinite_image_browser_dir, \"main\")\n",
        "    clone_repo(discordia_archivum_url, discordia_archivum_dir, \"main\")\n",
        "\n",
        "    !pip install -q --upgrade -r {infinite_image_browser_deps}\n",
        "    !pip install python-dotenv\n",
        "    !pip install -q --upgrade -r {discordia_archivum_deps}\n",
        "\n",
        "def install_dependencies():\n",
        "    requirements_file = os.path.join(repo_dir, \"requirements.txt\")\n",
        "    model_util        = os.path.join(repo_dir, \"library/model_util.py\")\n",
        "    gpu_info          = getoutput('nvidia-smi')\n",
        "    t4_xformers_wheel = \"https://github.com/Linaqruf/colab-xformers/releases/download/0.0.20/xformers-0.0.20+1d635e1.d20230519-cp310-cp310-linux_x86_64.whl\"\n",
        "\n",
        "    !apt install aria2 lz4\n",
        "    !wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "    !pip install -q --upgrade -r {requirements_file}\n",
        "\n",
        "    if '2.0.1+cu118' in torch.__version__:\n",
        "        if 'T4' in gpu_info:\n",
        "            !pip install -q {t4_xformers_wheel}\n",
        "        else:\n",
        "            !pip install -q xformers==0.0.20\n",
        "    else:\n",
        "        !pip install -q torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 torchtext==0.15.1 torchdata==0.6.0 --extra-index-url https://download.pytorch.org/whl/cu118 -U\n",
        "        !pip install -q xformers==0.0.19 triton==2.0.0 -U\n",
        "\n",
        "    from accelerate.utils import write_basic_config\n",
        "\n",
        "    if not os.path.exists(accelerate_config):\n",
        "        write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "def prepare_environment():\n",
        "    os.environ[\"LD_PRELOAD\"] = \"/content/libtcmalloc_minimal.so.4\"\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "\n",
        "def main():\n",
        "    os.chdir(root_dir)\n",
        "    clone_repo(repo_url, repo_dir, branch)\n",
        "    os.chdir(repo_dir)\n",
        "    setup_directories()\n",
        "    install_repository()\n",
        "    install_dependencies()\n",
        "    prepare_environment()\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H_MUxFeoWEJh"
      },
      "outputs": [],
      "source": [
        "#@title **Download Pretrained Model**\n",
        "import os\n",
        "\n",
        "download_link     = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.safetensors\" #@param {type:\"string\"}\n",
        "folder_path       = \"/content/pretrained_model\" #@param {type:\"string\"}\n",
        "file_name = download_link.split(\"/\")[-1]\n",
        "vae_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "!wget {download_link} -P {folder_path}\n",
        "print(\"Path variabel vae_path:\", vae_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kh7CeDqK4l3Y"
      },
      "outputs": [],
      "source": [
        "# @title ## **1.3. Directory Config**\n",
        "# @markdown Specify the location of your training data in the following cell. A folder with the same name as your input will be created.\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "train_data_dir = \"/content/LoRA/train_data\"  # @param {'type' : 'string'}\n",
        "main_folder = train_data_dir\n",
        "\n",
        "%store train_data_dir\n",
        "%store main_folder\n",
        "\n",
        "os.makedirs(train_data_dir, exist_ok=True)\n",
        "print(f\"Your train data directory : {train_data_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KNtFNAzPPpuS"
      },
      "outputs": [],
      "source": [
        "# @title ## **1.3. Directory Config**\n",
        "# @markdown Specify the location of your training data in the following cell. A folder with the same name as your input will be created.\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "part_data = \"Part1\"  # @param {'type' : 'string'}\n",
        "%store part_data\n",
        "\n",
        "print(f\"Your train data directory : {part_data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aTsSfsQj1nQT"
      },
      "outputs": [],
      "source": [
        "# @title ## **2.1. Unzip Dataset**\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "from urllib.parse import unquote\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# @title ## Unzip Dataset\n",
        "# @markdown If your dataset is in a `zip` file and has been uploaded to a location, use this section to extract it.\n",
        "# @markdown The dataset will be downloaded and automatically extracted to `train_data_dir` if `unzip_to` is empty.\n",
        "\n",
        "zipfile_url = \"https://huggingface.co/datasets/Alterneko/a/resolve/main/processed/v2/Part[x]_v2.zip\"  # @param {type:\"string\"}\n",
        "unzip_to = \"\"  # @param {type:\"string\"}\n",
        "hf_token = \"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\"\n",
        "\n",
        "use_aria2c = True # @param {type:\"boolean\"}\n",
        "preserve_folders = False # @param {type:\"boolean\"}\n",
        "remove_after_unzipping = True # @param {type:\"boolean\"}\n",
        "\n",
        "if \"huggingface.co\" in zipfile_url and \"blob\" in zipfile_url:\n",
        "    zipfile_url = zipfile_url.replace(\"blob\", \"resolve\")\n",
        "\n",
        "if not unzip_to:\n",
        "    unzip_to = train_data_dir\n",
        "\n",
        "def get_filename_from_url(url):\n",
        "    if \"huggingface.co\" or \"/content/\" in url:\n",
        "        return os.path.basename(url)\n",
        "\n",
        "    response = requests.head(url, allow_redirects=True)\n",
        "    cd = response.headers.get('content-disposition')\n",
        "    if cd:\n",
        "        fname = re.findall('filename=(.+)', cd)\n",
        "        if len(fname) == 0:\n",
        "            return \"zipfile.zip\"\n",
        "        return unquote(fname[0])\n",
        "\n",
        "    return \"zipfile.zip\"\n",
        "\n",
        "def download_with_requests(url, output_path):\n",
        "    print(f\"Downloading {url} with requests...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(output_path, 'wb') as file:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            file.write(chunk)\n",
        "    print(f\"Downloaded to {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def download_with_aria2c(url, output_path):\n",
        "    print(f\"Downloading {url} with aria2c...\")\n",
        "    aria_args = {\n",
        "        'console-log-level': 'error',\n",
        "        'summary-interval': '10',\n",
        "        'continue': 'true',\n",
        "        'max-connection-per-server': '16',\n",
        "        'min-split-size': '1M',\n",
        "        'split': '16',\n",
        "        'dir': os.path.dirname(output_path),\n",
        "        'out': os.path.basename(output_path),\n",
        "    }\n",
        "\n",
        "    if \"huggingface.co\" in url:\n",
        "        aria_args['header'] = f\"Authorization: Bearer {hf_token}\"\n",
        "\n",
        "    cmd = ['aria2c'] + [f'--{k}={v}' for k, v in aria_args.items()] + [url]\n",
        "    subprocess.run(cmd)\n",
        "    print(f\"Downloaded to {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def move_files(train_dir):\n",
        "    for filename in os.listdir(train_dir):\n",
        "        file_path = os.path.join(train_dir, filename)\n",
        "        if filename.startswith(\"meta_\") and filename.endswith(\".json\"):\n",
        "            if not os.path.exists(file_path):\n",
        "                shutil.move(file_path, training_dir)\n",
        "            else:\n",
        "                os.remove(file_path)\n",
        "\n",
        "def remove_empty_dirs(path):\n",
        "    for dirpath, dirnames, files in os.walk(path, topdown=False):  # start from leaf folders\n",
        "        for dirname in dirnames:\n",
        "            full_dir_path = os.path.join(dirpath, dirname)\n",
        "            if not os.listdir(full_dir_path):  # Check if directory is empty\n",
        "                os.rmdir(full_dir_path)\n",
        "                print(f\"Removed empty directory: {full_dir_path}\")\n",
        "\n",
        "def extract_dataset(zip_file, output_path):\n",
        "    with ZipFile(zip_file, 'r') as zip_ref:\n",
        "        print(f\"Extracting {zip_file} to {output_path}...\")\n",
        "\n",
        "        if not preserve_folders:  # If we do not want to preserve folder structure\n",
        "            for member in zip_ref.namelist():\n",
        "                # Extract only the file name, discard directory structure\n",
        "                filename = os.path.basename(member)\n",
        "                if filename:  # Check if file name is not empty (this skips directories)\n",
        "                    zip_ref.extract(member, output_path)\n",
        "                    source_path = os.path.join(output_path, member)\n",
        "                    target_path = os.path.join(output_path, filename)\n",
        "                    os.rename(source_path, target_path)\n",
        "\n",
        "            remove_empty_dirs(output_path)\n",
        "\n",
        "        else:\n",
        "            zip_ref.extractall(output_path)\n",
        "\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "def download_dataset(url, output_path):\n",
        "    if url.startswith(\"/content\"):\n",
        "        print(f\"Using file at {url}\")\n",
        "        return url\n",
        "\n",
        "    elif \"drive.google.com\" in url:\n",
        "        print(\"Downloading from Google Drive...\")\n",
        "        cmd = ['gdown', '--id', url.split('/')[-2], '-O', output_path]\n",
        "        subprocess.run(cmd)\n",
        "        return output_path\n",
        "\n",
        "    elif use_aria2c:\n",
        "        return download_with_aria2c(url, output_path)\n",
        "\n",
        "    else:\n",
        "        return download_with_requests(url, output_path)\n",
        "\n",
        "def main():\n",
        "    zipfile_name = get_filename_from_url(zipfile_url)\n",
        "    output_path = os.path.join(root_dir, zipfile_name)\n",
        "\n",
        "    zip_file = download_dataset(zipfile_url, output_path)\n",
        "\n",
        "    extract_dataset(zip_file, unzip_to)\n",
        "\n",
        "    move_files(unzip_to)\n",
        "\n",
        "    if remove_after_unzipping and \"/content/drive\" not in zip_file:\n",
        "        os.remove(zip_file)\n",
        "        print(f\"Removed {zip_file}\")\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jz2emq6vWnPu"
      },
      "outputs": [],
      "source": [
        "# @title ## **3.1. Data Cleaning**\n",
        "import os\n",
        "import random\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "test = os.listdir(train_data_dir)\n",
        "#@markdown This section removes unsupported media types such as `.mp4`, `.webm`, and `.gif`, as well as any unnecessary files.\n",
        "#@markdown To convert a transparent dataset with an alpha channel (RGBA) to RGB and give it a white background, set the `convert` parameter to `True`.\n",
        "convert = True  # @param {type:\"boolean\"}\n",
        "#@markdown Alternatively, you can give the background a `random_color` instead of white by checking the corresponding option.\n",
        "random_color = False  # @param {type:\"boolean\"}\n",
        "recursive = False\n",
        "\n",
        "batch_size = 32\n",
        "supported_types = [\n",
        "    \".png\",\n",
        "    \".jpg\",\n",
        "    \".jpeg\",\n",
        "    \".webp\",\n",
        "    \".bmp\",\n",
        "    \".json\",\n",
        "]\n",
        "\n",
        "background_colors = [\n",
        "    (255, 255, 255),\n",
        "    (0, 0, 0),\n",
        "    (255, 0, 0),\n",
        "    (0, 255, 0),\n",
        "    (0, 0, 255),\n",
        "    (255, 255, 0),\n",
        "    (255, 0, 255),\n",
        "    (0, 255, 255),\n",
        "]\n",
        "\n",
        "def clean_directory(directory):\n",
        "    for item in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, item)\n",
        "        if os.path.isfile(file_path):\n",
        "            file_ext = os.path.splitext(item)[1]\n",
        "            if file_ext not in supported_types:\n",
        "#                print(f\"Deleting file {item} from {directory}\")\n",
        "                os.remove(file_path)\n",
        "        elif os.path.isdir(file_path) and recursive:\n",
        "            clean_directory(file_path)\n",
        "\n",
        "def process_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    img_dir, image_name = os.path.split(image_path)\n",
        "\n",
        "    if img.mode in (\"RGBA\", \"LA\"):\n",
        "        if random_color:\n",
        "            background_color = random.choice(background_colors)\n",
        "        else:\n",
        "            background_color = (255, 255, 255)\n",
        "        bg = Image.new(\"RGB\", img.size, background_color)\n",
        "        bg.paste(img, mask=img.split()[-1])\n",
        "\n",
        "        if image_name.endswith(\".webp\"):\n",
        "            bg = bg.convert(\"RGB\")\n",
        "            new_image_path = os.path.join(img_dir, image_name.replace(\".webp\", \".jpg\"))\n",
        "            bg.save(new_image_path, \"JPEG\")\n",
        "            os.remove(image_path)\n",
        "            print(f\" Converted image: {image_name} to {os.path.basename(new_image_path)}\")\n",
        "        else:\n",
        "            bg.save(image_path, \"PNG\")\n",
        "            print(f\" Converted image: {image_name}\")\n",
        "    else:\n",
        "        if image_name.endswith(\".webp\"):\n",
        "            new_image_path = os.path.join(img_dir, image_name.replace(\".webp\", \".jpg\"))\n",
        "            img.save(new_image_path, \"JPEG\")\n",
        "            os.remove(image_path)\n",
        "            print(f\" Converted image: {image_name} to {os.path.basename(new_image_path)}\")\n",
        "        else:\n",
        "            img.save(image_path, \"PNG\")\n",
        "\n",
        "def find_images(directory):\n",
        "    images = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".png\") or file.endswith(\".webp\"):\n",
        "                images.append(os.path.join(root, file))\n",
        "    return images\n",
        "\n",
        "clean_directory(train_data_dir)\n",
        "images = find_images(train_data_dir)\n",
        "num_batches = len(images) // batch_size + 1\n",
        "\n",
        "if convert:\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        for i in tqdm(range(num_batches)):\n",
        "            start = i * batch_size\n",
        "            end = start + batch_size\n",
        "            batch = images[start:end]\n",
        "            executor.map(process_image, batch)\n",
        "\n",
        "    print(\"All images have been converted\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **Move Lowres Data**\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def move_small_files(directory, max_size_kb, dest_folder):\n",
        "    moved_count = 0\n",
        "    if not os.path.exists(dest_folder):\n",
        "       os.makedirs(dest_folder)\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_size_kb = os.path.getsize(file_path) / 1024  # Convert to KB\n",
        "            if file_size_kb < max_size_kb:\n",
        "                new_location = os.path.join(dest_folder, file)\n",
        "                shutil.move(file_path, new_location)\n",
        "                moved_count += 1\n",
        "    return moved_count\n",
        "\n",
        "train_data_dir = \"/content/LoRA/train_data\" # @param {type:\"string\"}\n",
        "max_file_size_kb = 50 # @param {type:\"number\"}\n",
        "destination_folder = \"/content/LoRA/lowres_Data\" # @param {type:\"string\"}\n",
        "\n",
        "total_moved = move_small_files(train_data_dir, max_file_size_kb, destination_folder)\n",
        "print(f\"Total files moved: {total_moved}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Vppni6v--ZoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **Tagging for NSFW Filted**\n",
        "import os\n",
        "\n",
        "%cd /content/kohya-trainer\n",
        "!wget https://huggingface.co/datasets/Alterneko/Reserval/resolve/main/waifu.py\n",
        "use_gpu = True #@param {type:\"boolean\"}\n",
        "\n",
        "!python waifu.py --img_dir {train_data_dir} --use_gpu"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DVgbSB81qXoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **Split Dataset to Right Folder**\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def organize_files(input_folder, output_folder):\n",
        "    safe_folder = os.path.join(output_folder, \"safe\")\n",
        "    suggestive_folder = os.path.join(output_folder, \"suggestive\")\n",
        "    explicit_folder = os.path.join(output_folder, \"explicit\")\n",
        "\n",
        "    # Membuat subfolder jika belum ada\n",
        "    for folder in [safe_folder, suggestive_folder, explicit_folder]:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    # Mendapatkan daftar file dalam folder input\n",
        "    files = os.listdir(input_folder)\n",
        "\n",
        "    for file in files:\n",
        "        if \"safe\" in file:\n",
        "            shutil.move(os.path.join(input_folder, file), os.path.join(safe_folder, file))\n",
        "        elif \"suggestive\" in file:\n",
        "            shutil.move(os.path.join(input_folder, file), os.path.join(suggestive_folder, file))\n",
        "        elif \"explicit\" in file:\n",
        "            shutil.move(os.path.join(input_folder, file), os.path.join(explicit_folder, file))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_folder = train_data_dir   # Ganti dengan path folder input yang sesuai\n",
        "    output_folder = \"/content/filtered\"  #@param {type:\"string\"}\n",
        "    organize_files(input_folder, output_folder)\n",
        "    print(\"Pemindahan file selesai.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xfo0r4Dlp6tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YcL3-BBvVg3M"
      },
      "outputs": [],
      "source": [
        "#@title ## **Rename Dataset with Random Name and Prefix**\n",
        "%store -r\n",
        "import os\n",
        "import secrets\n",
        "\n",
        "# Ganti nilai folder_to_rename dan prefix_to_add sesuai kebutuhan Anda\n",
        "part_to_rename = \"[x]\" #@param {type:\"string\"}\n",
        "type_to_rename = \"explicit\" #@param [\"safe\", \"explicit\", \"suggestive\"]\n",
        "folder_to_rename = \"/content/filtered/\" + type_to_rename\n",
        "prefix_to_add = \"v3_\" + part_data + \"_\" + type_to_rename\n",
        "\n",
        "def generate_random_string(length):\n",
        "    return secrets.token_hex(length // 2)  # Karena setiap byte akan diubah menjadi dua karakter hex\n",
        "\n",
        "def rename_files_in_folder(folder_path, prefix):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if os.path.isfile(os.path.join(folder_path, filename)):\n",
        "            file_extension = os.path.splitext(filename)[1]\n",
        "            random_string = generate_random_string(10)\n",
        "            new_filename = f\"{prefix}_{random_string}{file_extension}\"\n",
        "            os.rename(os.path.join(folder_path, filename), os.path.join(folder_path, new_filename))\n",
        "\n",
        "target_folder = folder_to_rename\n",
        "prefix = prefix_to_add\n",
        "\n",
        "rename_files_in_folder(target_folder, prefix)\n",
        "print(\"Files renamed successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4JZrEGdOAs69"
      },
      "outputs": [],
      "source": [
        "#@title ## **Rename Dataset with Random Name and Prefix**\n",
        "%store -r\n",
        "import os\n",
        "import secrets\n",
        "\n",
        "# Ganti nilai folder_to_rename dan prefix_to_add sesuai kebutuhan Anda\n",
        "part_to_rename = \"[x]\" #@param {type:\"string\"}\n",
        "type_to_rename = \"safe\" #@param [\"safe\", \"explicit\", \"suggestive\"]\n",
        "folder_to_rename = \"/content/filtered/\" + type_to_rename\n",
        "prefix_to_add = \"v3_\" + part_data + \"_\" + type_to_rename\n",
        "\n",
        "def generate_random_string(length):\n",
        "    return secrets.token_hex(length // 2)  # Karena setiap byte akan diubah menjadi dua karakter hex\n",
        "\n",
        "def rename_files_in_folder(folder_path, prefix):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if os.path.isfile(os.path.join(folder_path, filename)):\n",
        "            file_extension = os.path.splitext(filename)[1]\n",
        "            random_string = generate_random_string(10)\n",
        "            new_filename = f\"{prefix}_{random_string}{file_extension}\"\n",
        "            os.rename(os.path.join(folder_path, filename), os.path.join(folder_path, new_filename))\n",
        "\n",
        "target_folder = folder_to_rename\n",
        "prefix = prefix_to_add\n",
        "\n",
        "rename_files_in_folder(target_folder, prefix)\n",
        "print(\"Files renamed successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tkQTpddpA58U"
      },
      "outputs": [],
      "source": [
        "#@title ## **Rename Dataset with Random Name and Prefix**\n",
        "%store -r\n",
        "import os\n",
        "import secrets\n",
        "\n",
        "# Ganti nilai folder_to_rename dan prefix_to_add sesuai kebutuhan Anda\n",
        "part_to_rename = \"[x]\" #@param {type:\"string\"}\n",
        "type_to_rename = \"suggestive\" #@param [\"safe\", \"explicit\", \"suggestive\"]\n",
        "folder_to_rename = \"/content/filtered/\" + type_to_rename\n",
        "prefix_to_add = \"v3_\" + part_data + \"_\" + type_to_rename\n",
        "\n",
        "def generate_random_string(length):\n",
        "    return secrets.token_hex(length // 2)  # Karena setiap byte akan diubah menjadi dua karakter hex\n",
        "\n",
        "def rename_files_in_folder(folder_path, prefix):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if os.path.isfile(os.path.join(folder_path, filename)):\n",
        "            file_extension = os.path.splitext(filename)[1]\n",
        "            random_string = generate_random_string(10)\n",
        "            new_filename = f\"{prefix}_{random_string}{file_extension}\"\n",
        "            os.rename(os.path.join(folder_path, filename), os.path.join(folder_path, new_filename))\n",
        "\n",
        "target_folder = folder_to_rename\n",
        "prefix = prefix_to_add\n",
        "\n",
        "rename_files_in_folder(target_folder, prefix)\n",
        "print(\"Files renamed successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-BdXV7rAy2ag"
      },
      "outputs": [],
      "source": [
        "#@title ### **3.2.2. Waifu Diffusion 1.4 Tagger V2**\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "\n",
        "tagging_dir = \"/content/filtered\" #@param {type:\"string\"}\n",
        "#@markdown [Waifu Diffusion 1.4 Tagger V2](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) is a Danbooru-styled image classification model developed by SmilingWolf. It can also be useful for general image tagging, for example, `1girl, solo, looking_at_viewer, short_hair, bangs, simple_background`.\n",
        "model = \"SmilingWolf/wd-v1-4-moat-tagger-v2\" #@param [\"SmilingWolf/wd-v1-4-moat-tagger-v2\", \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\", \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\", \"SmilingWolf/wd-v1-4-convnext-tagger-v2\", \"SmilingWolf/wd-v1-4-vit-tagger-v2\"]\n",
        "#@markdown Separate `undesired_tags` with comma `(,)` if you want to remove multiple tags, e.g. `1girl,solo,smile`.\n",
        "undesired_tags = \"realistic,cosplay\" #@param {type:'string'}\n",
        "#@markdown Adjust `general_threshold` for pruning tags (less tags, less flexible). `character_threshold` is useful if you want to train with character tags, e.g. `hakurei reimu`.\n",
        "general_threshold = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "character_threshold = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "config = {\n",
        "    \"_train_data_dir\"           : tagging_dir,\n",
        "    \"batch_size\"                : 8,\n",
        "    \"repo_id\"                   : model,\n",
        "    \"recursive\"                 : True,\n",
        "    \"remove_underscore\"         : True,\n",
        "    \"general_threshold\"         : general_threshold,\n",
        "    \"character_threshold\"       : character_threshold,\n",
        "    \"caption_extension\"         : \".txt\",\n",
        "    \"max_data_loader_n_workers\" : 2,\n",
        "    \"debug\"                     : True,\n",
        "    \"undesired_tags\"            : undesired_tags\n",
        "}\n",
        "\n",
        "args = \"\"\n",
        "for k, v in config.items():\n",
        "    if k.startswith(\"_\"):\n",
        "        args += f'\"{v}\" '\n",
        "    elif isinstance(v, str):\n",
        "        args += f'--{k}=\"{v}\" '\n",
        "    elif isinstance(v, bool) and v:\n",
        "        args += f\"--{k} \"\n",
        "    elif isinstance(v, float) and not isinstance(v, bool):\n",
        "        args += f\"--{k}={v} \"\n",
        "    elif isinstance(v, int) and not isinstance(v, bool):\n",
        "        args += f\"--{k}={v} \"\n",
        "\n",
        "final_args = f\"python tag_images_by_wd14_tagger.py {args}\"\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "!{final_args}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_mLVURhM9PFE"
      },
      "outputs": [],
      "source": [
        "# @title ### **3.2.3. Custom Caption/Tag (FOR BACKUP, NOT USED)**\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "type_to_tag = \"safe\" #@param [\"safe\", \"explicit\", \"suggestive\"]\n",
        "# @markdown Add or remove custom tags here.\n",
        "extension   = \".txt\"  # @param [\".txt\", \".caption\"]\n",
        "custom_tag  = \"\"  # @param {type:\"string\"}\n",
        "# @markdown Use `sub_folder` option to specify a subfolder for multi-concept training.\n",
        "# @markdown > Specify `--all` to process all subfolders/`recursive`\n",
        "sub_folder  = \"/content/filtered/\" #@param {type: \"string\"}\n",
        "# @markdown Enable this to append custom tags at the end of lines.\n",
        "append      = False  # @param {type:\"boolean\"}\n",
        "# @markdown Enable this if you want to remove captions/tags instead.\n",
        "remove_tag  = False  # @param {type:\"boolean\"}\n",
        "recursive   = False\n",
        "\n",
        "if type_to_tag == \"safe\":\n",
        "  custom_tag = \"1girl\"\n",
        "  sub_folder = sub_folder + type_to_tag\n",
        "elif type_to_tag == \"explicit\":\n",
        "  custom_tag = \"nsfw\"\n",
        "  sub_folder = sub_folder + type_to_tag\n",
        "elif type_to_tag == \"suggestive\":\n",
        "  custom_tag = \"sexually suggestive\"\n",
        "  sub_folder = sub_folder + type_to_tag\n",
        "\n",
        "if sub_folder == \"\":\n",
        "    image_dir = train_data_dir\n",
        "elif sub_folder == \"--all\":\n",
        "    image_dir = train_data_dir\n",
        "    recursive = True\n",
        "elif sub_folder.startswith(\"/content\"):\n",
        "    image_dir = sub_folder\n",
        "else:\n",
        "    image_dir = os.path.join(train_data_dir, sub_folder)\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "    return contents\n",
        "\n",
        "def write_file(filename, contents):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "def process_tags(filename, custom_tag, append, remove_tag):\n",
        "    contents = read_file(filename)\n",
        "    tags = [tag.strip() for tag in contents.split(',')]\n",
        "    custom_tags = [tag.strip() for tag in custom_tag.split(',')]\n",
        "\n",
        "    for custom_tag in custom_tags:\n",
        "        custom_tag = custom_tag.replace(\"_\", \" \")\n",
        "        if remove_tag:\n",
        "            while custom_tag in tags:\n",
        "                tags.remove(custom_tag)\n",
        "        else:\n",
        "            if custom_tag not in tags:\n",
        "                if append:\n",
        "                    tags.append(custom_tag)\n",
        "                else:\n",
        "                    tags.insert(0, custom_tag)\n",
        "\n",
        "    contents = ', '.join(tags)\n",
        "    write_file(filename, contents)\n",
        "\n",
        "def process_directory(image_dir, tag, append, remove_tag, recursive):\n",
        "    for filename in os.listdir(image_dir):\n",
        "        file_path = os.path.join(image_dir, filename)\n",
        "\n",
        "        if os.path.isdir(file_path) and recursive:\n",
        "            process_directory(file_path, tag, append, remove_tag, recursive)\n",
        "        elif filename.endswith(extension):\n",
        "            process_tags(file_path, tag, append, remove_tag)\n",
        "\n",
        "tag = custom_tag\n",
        "\n",
        "if not any(\n",
        "    [filename.endswith(extension) for filename in os.listdir(image_dir)]\n",
        "):\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")):\n",
        "            open(\n",
        "                os.path.join(image_dir, filename.split(\".\")[0] + extension),\n",
        "                \"w\",\n",
        "            ).close()\n",
        "\n",
        "if custom_tag:\n",
        "    process_directory(image_dir, tag, append, remove_tag, recursive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q_PHGHciCxLY"
      },
      "outputs": [],
      "source": [
        "# @title ### **3.2.3. Custom Caption/Tag (FOR BACKUP, NOT USED)**\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "type_to_tag = \"explicit\" #@param [\"safe\", \"explicit\", \"suggestive\"]\n",
        "# @markdown Add or remove custom tags here.\n",
        "extension   = \".txt\"  # @param [\".txt\", \".caption\"]\n",
        "custom_tag  = \"\"  # @param {type:\"string\"}\n",
        "# @markdown Use `sub_folder` option to specify a subfolder for multi-concept training.\n",
        "# @markdown > Specify `--all` to process all subfolders/`recursive`\n",
        "sub_folder  = \"/content/filtered/\" #@param {type: \"string\"}\n",
        "# @markdown Enable this to append custom tags at the end of lines.\n",
        "append      = False  # @param {type:\"boolean\"}\n",
        "# @markdown Enable this if you want to remove captions/tags instead.\n",
        "remove_tag  = False  # @param {type:\"boolean\"}\n",
        "recursive   = False\n",
        "\n",
        "if type_to_tag == \"safe\":\n",
        "  custom_tag = \"1girl\"\n",
        "  sub_folder = sub_folder + type_to_tag\n",
        "elif type_to_tag == \"explicit\":\n",
        "  custom_tag = \"nsfw\"\n",
        "  sub_folder = sub_folder + type_to_tag\n",
        "elif type_to_tag == \"suggestive\":\n",
        "  custom_tag = \"sexually suggestive\"\n",
        "  sub_folder = sub_folder + type_to_tag\n",
        "\n",
        "if sub_folder == \"\":\n",
        "    image_dir = train_data_dir\n",
        "elif sub_folder == \"--all\":\n",
        "    image_dir = train_data_dir\n",
        "    recursive = True\n",
        "elif sub_folder.startswith(\"/content\"):\n",
        "    image_dir = sub_folder\n",
        "else:\n",
        "    image_dir = os.path.join(train_data_dir, sub_folder)\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "    return contents\n",
        "\n",
        "def write_file(filename, contents):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "def process_tags(filename, custom_tag, append, remove_tag):\n",
        "    contents = read_file(filename)\n",
        "    tags = [tag.strip() for tag in contents.split(',')]\n",
        "    custom_tags = [tag.strip() for tag in custom_tag.split(',')]\n",
        "\n",
        "    for custom_tag in custom_tags:\n",
        "        custom_tag = custom_tag.replace(\"_\", \" \")\n",
        "        if remove_tag:\n",
        "            while custom_tag in tags:\n",
        "                tags.remove(custom_tag)\n",
        "        else:\n",
        "            if custom_tag not in tags:\n",
        "                if append:\n",
        "                    tags.append(custom_tag)\n",
        "                else:\n",
        "                    tags.insert(0, custom_tag)\n",
        "\n",
        "    contents = ', '.join(tags)\n",
        "    write_file(filename, contents)\n",
        "\n",
        "def process_directory(image_dir, tag, append, remove_tag, recursive):\n",
        "    for filename in os.listdir(image_dir):\n",
        "        file_path = os.path.join(image_dir, filename)\n",
        "\n",
        "        if os.path.isdir(file_path) and recursive:\n",
        "            process_directory(file_path, tag, append, remove_tag, recursive)\n",
        "        elif filename.endswith(extension):\n",
        "            process_tags(file_path, tag, append, remove_tag)\n",
        "\n",
        "tag = custom_tag\n",
        "\n",
        "if not any(\n",
        "    [filename.endswith(extension) for filename in os.listdir(image_dir)]\n",
        "):\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")):\n",
        "            open(\n",
        "                os.path.join(image_dir, filename.split(\".\")[0] + extension),\n",
        "                \"w\",\n",
        "            ).close()\n",
        "\n",
        "if custom_tag:\n",
        "    process_directory(image_dir, tag, append, remove_tag, recursive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-6T0RlXUCxUo"
      },
      "outputs": [],
      "source": [
        "# @title ### **3.2.3. Custom Caption/Tag (FOR BACKUP, NOT USED)**\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "type_to_tag = \"suggestive\" #@param [\"safe\", \"explicit\", \"suggestive\"]\n",
        "# @markdown Add or remove custom tags here.\n",
        "extension   = \".txt\"  # @param [\".txt\", \".caption\"]\n",
        "custom_tag  = \"\"  # @param {type:\"string\"}\n",
        "# @markdown Use `sub_folder` option to specify a subfolder for multi-concept training.\n",
        "# @markdown > Specify `--all` to process all subfolders/`recursive`\n",
        "sub_folder  = \"/content/filtered/\" #@param {type: \"string\"}\n",
        "# @markdown Enable this to append custom tags at the end of lines.\n",
        "append      = False  # @param {type:\"boolean\"}\n",
        "# @markdown Enable this if you want to remove captions/tags instead.\n",
        "remove_tag  = False  # @param {type:\"boolean\"}\n",
        "recursive   = False\n",
        "\n",
        "if type_to_tag == \"safe\":\n",
        "  custom_tag = \"1girl\"\n",
        "  sub_folder = sub_folder + type_to_tag\n",
        "elif type_to_tag == \"explicit\":\n",
        "  custom_tag = \"nsfw\"\n",
        "  sub_folder = sub_folder + type_to_tag\n",
        "elif type_to_tag == \"suggestive\":\n",
        "  custom_tag = \"sexually suggestive\"\n",
        "  sub_folder = sub_folder + type_to_tag\n",
        "\n",
        "if sub_folder == \"\":\n",
        "    image_dir = train_data_dir\n",
        "elif sub_folder == \"--all\":\n",
        "    image_dir = train_data_dir\n",
        "    recursive = True\n",
        "elif sub_folder.startswith(\"/content\"):\n",
        "    image_dir = sub_folder\n",
        "else:\n",
        "    image_dir = os.path.join(train_data_dir, sub_folder)\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "    return contents\n",
        "\n",
        "def write_file(filename, contents):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "def process_tags(filename, custom_tag, append, remove_tag):\n",
        "    contents = read_file(filename)\n",
        "    tags = [tag.strip() for tag in contents.split(',')]\n",
        "    custom_tags = [tag.strip() for tag in custom_tag.split(',')]\n",
        "\n",
        "    for custom_tag in custom_tags:\n",
        "        custom_tag = custom_tag.replace(\"_\", \" \")\n",
        "        if remove_tag:\n",
        "            while custom_tag in tags:\n",
        "                tags.remove(custom_tag)\n",
        "        else:\n",
        "            if custom_tag not in tags:\n",
        "                if append:\n",
        "                    tags.append(custom_tag)\n",
        "                else:\n",
        "                    tags.insert(0, custom_tag)\n",
        "\n",
        "    contents = ', '.join(tags)\n",
        "    write_file(filename, contents)\n",
        "\n",
        "def process_directory(image_dir, tag, append, remove_tag, recursive):\n",
        "    for filename in os.listdir(image_dir):\n",
        "        file_path = os.path.join(image_dir, filename)\n",
        "\n",
        "        if os.path.isdir(file_path) and recursive:\n",
        "            process_directory(file_path, tag, append, remove_tag, recursive)\n",
        "        elif filename.endswith(extension):\n",
        "            process_tags(file_path, tag, append, remove_tag)\n",
        "\n",
        "tag = custom_tag\n",
        "\n",
        "if not any(\n",
        "    [filename.endswith(extension) for filename in os.listdir(image_dir)]\n",
        "):\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")):\n",
        "            open(\n",
        "                os.path.join(image_dir, filename.split(\".\")[0] + extension),\n",
        "                \"w\",\n",
        "            ).close()\n",
        "\n",
        "if custom_tag:\n",
        "    process_directory(image_dir, tag, append, remove_tag, recursive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hhgatqF3leHJ"
      },
      "outputs": [],
      "source": [
        "# @title ## **3.4. Bucketing and Latents Caching**\n",
        "%store -r\n",
        "\n",
        "data_type = \"safe\" #@param [\"explicit\", \"safe\", \"suggestive\"]\n",
        "\n",
        "%mkdir \"/content/meta/\"\n",
        "meta_dir = \"/content/meta/\"\n",
        "\n",
        "filtered_dir = \"/content/filtered/\" + data_type\n",
        "# @markdown This code will create buckets based on the `bucket_resolution` provided for multi-aspect ratio training, and then convert all images within the `train_data_dir` to latents.\n",
        "bucketing_json    = os.path.join(meta_dir, f\"{data_type}_meta_lat.json\")\n",
        "metadata_json     = os.path.join(meta_dir, f\"{data_type}_meta_clean.json\")\n",
        "bucket_resolution = 512  # @param {type:\"slider\", min:512, max:1024, step:128}\n",
        "mixed_precision   = \"no\"  # @param [\"no\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "skip_existing     = False  # @param{type:\"boolean\"}\n",
        "flip_aug          = True  # @param{type:\"boolean\"}\n",
        "# @markdown Use `clean_caption` option to clean such as duplicate tags, `women` to `girl`, etc\n",
        "clean_caption     = True #@param {type:\"boolean\"}\n",
        "#@markdown Use the `recursive` option to process subfolders as well\n",
        "recursive         = False #@param {type:\"boolean\"}\n",
        "\n",
        "metadata_config = {\n",
        "    \"_train_data_dir\": filtered_dir,\n",
        "    \"_out_json\": metadata_json,\n",
        "    \"recursive\": recursive,\n",
        "    \"full_path\": recursive,\n",
        "    \"clean_caption\": clean_caption\n",
        "}\n",
        "\n",
        "bucketing_config = {\n",
        "    \"_train_data_dir\": filtered_dir,\n",
        "    \"_in_json\": metadata_json,\n",
        "    \"_out_json\": bucketing_json,\n",
        "    \"_model_name_or_path\": vae_path if vae_path else model_path,\n",
        "    \"recursive\": recursive,\n",
        "    \"full_path\": recursive,\n",
        "    \"flip_aug\": flip_aug,\n",
        "    \"skip_existing\": skip_existing,\n",
        "    \"batch_size\": 4,\n",
        "    \"max_data_loader_n_workers\": 2,\n",
        "    \"max_resolution\": f\"{bucket_resolution}, {bucket_resolution}\",\n",
        "    \"mixed_precision\": mixed_precision,\n",
        "}\n",
        "\n",
        "def generate_args(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "    return args.strip()\n",
        "\n",
        "merge_metadata_args = generate_args(metadata_config)\n",
        "prepare_buckets_args = generate_args(bucketing_config)\n",
        "\n",
        "merge_metadata_command = f\"python merge_all_to_metadata.py {merge_metadata_args}\"\n",
        "prepare_buckets_command = f\"python prepare_buckets_latents.py {prepare_buckets_args}\"\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "!{merge_metadata_command}\n",
        "time.sleep(1)\n",
        "!{prepare_buckets_command}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "D2e2rsobHT5F"
      },
      "outputs": [],
      "source": [
        "#@title Zip Safe Dataset\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_type = \"safe\" #@param [\"explicit\", \"safe\", \"suggestive\"]\n",
        "source_folder = \"/content/filtered/\" #@param {type:\"string\"}\n",
        "source_folder_to_zip = source_folder + zip_type\n",
        "output_dir = \"/content/LoRA\" #@param {type:\"string\"}\n",
        "output_name = zip_type\n",
        "#zip_part = \"[x]\" #@param {type:\"string\"}\n",
        "\n",
        "if output_name:\n",
        "  output_name = part_data + \"_\" + zip_type\n",
        "else:\n",
        "  output_name = prefix_to_add\n",
        "\n",
        "output_path_for_zip = f'{output_dir}/{output_name}.zip'\n",
        "safe_dataset_zip = output_path_for_zip\n",
        "\n",
        "def zip_folder_contents(folder_path, output_zip_path):\n",
        "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, folder_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    source_folder = source_folder_to_zip\n",
        "    output_zip = output_path_for_zip\n",
        "\n",
        "    zip_folder_contents(source_folder, output_zip)\n",
        "    print(\"Folder contents compressed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qqvJZLzUEb7c"
      },
      "outputs": [],
      "source": [
        "#@title Push Safe Zip to Huggingface\n",
        "\n",
        "import os\n",
        "from huggingface_hub import upload_file, create_repo\n",
        "\n",
        "file_path = \"\" #@param {type:\"string\"}\n",
        "prefix = \"\" #@param {type:\"string\"}\n",
        "\n",
        "hf_token = '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' #@param {type:\"string\"}\n",
        "repo_id = 'Alterneko/a' #@param {type:\"string\"}\n",
        "repo_type = 'dataset' #@param ['model','dataset']\n",
        "commit_message = 'Upload' #@param {type:\"string\"}\n",
        "make_private = False #@param {type:\"boolean\"}\n",
        "\n",
        "version_data = \"v3\" #@param {type:\"string\"}\n",
        "zip_type = \"processed\" #@param ['raw','processed','bucket']\n",
        "\n",
        "if file_path == \"\":\n",
        "  file_path = safe_dataset_zip\n",
        "else:\n",
        "  file_path = file_path\n",
        "\n",
        "if commit_message:\n",
        "  commit_message = commit_message\n",
        "else:\n",
        "  commit_message = f'Push {filename}'\n",
        "\n",
        "if version_data:\n",
        "  version = f'{version_data}/'\n",
        "else:\n",
        "  version = \"\"\n",
        "\n",
        "model_path = file_path\n",
        "filename = os.path.basename(model_path)\n",
        "final_filename = f'{filename}'\n",
        "path_in_repo = f'{zip_type}/{version}{final_filename}'\n",
        "\n",
        "create_repo(repo_id=repo_id, repo_type=repo_type, token=hf_token, exist_ok=True, private=make_private)\n",
        "upload_file(path_or_fileobj=model_path, path_in_repo=path_in_repo, repo_id=repo_id, repo_type=repo_type, commit_message=commit_message, token=hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0dU-qULKYnt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Push to HF.co\n",
        "\n",
        "import os\n",
        "from huggingface_hub import upload_file, create_repo\n",
        "\n",
        "file_path = \"/content/meta/safe_meta_lat.json\" #@param {type:\"string\"}\n",
        "hf_token = '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' #@param {type:\"string\"}\n",
        "repo_id = 'Alterneko/a' #@param {type:\"string\"}\n",
        "repo_type = 'dataset' #@param ['model','dataset']\n",
        "commit_message = 'Update' #@param {type:\"string\"}\n",
        "make_private = False #@param {type:\"boolean\"}\n",
        "\n",
        "version_data = \"v3\" #@param {type:\"string\"}\n",
        "zip_type = \"bucket\" #@param ['raw','processed','bucket']\n",
        "\n",
        "if commit_message:\n",
        "  commit_message = commit_message\n",
        "else:\n",
        "  commit_message = f'Push {filename}'\n",
        "\n",
        "if version_data:\n",
        "  version = f'{version_data}/'\n",
        "else:\n",
        "  version = \"\"\n",
        "\n",
        "model_path = file_path\n",
        "filename = os.path.basename(model_path)\n",
        "final_filename = f'{part_data}_{filename}'\n",
        "path_in_repo = f'{zip_type}/{version}{final_filename}'\n",
        "\n",
        "create_repo(repo_id=repo_id, repo_type=repo_type, token=hf_token, exist_ok=True, private=make_private)\n",
        "upload_file(path_or_fileobj=model_path, path_in_repo=path_in_repo, repo_id=repo_id, repo_type=repo_type, commit_message=commit_message, token=hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "40Vc66x7Eb5w"
      },
      "outputs": [],
      "source": [
        "# @title ## **3.4. Bucketing and Latents Caching**\n",
        "%store -r\n",
        "\n",
        "data_type = \"explicit\" #@param [\"explicit\", \"safe\", \"suggestive\"]\n",
        "\n",
        "%mkdir \"/content/meta/\"\n",
        "meta_dir = \"/content/meta/\"\n",
        "\n",
        "filtered_dir = \"/content/filtered/\" + data_type\n",
        "# @markdown This code will create buckets based on the `bucket_resolution` provided for multi-aspect ratio training, and then convert all images within the `train_data_dir` to latents.\n",
        "bucketing_json    = os.path.join(meta_dir, f\"{data_type}_meta_lat.json\")\n",
        "metadata_json     = os.path.join(meta_dir, f\"{data_type}_meta_clean.json\")\n",
        "bucket_resolution = 512  # @param {type:\"slider\", min:512, max:1024, step:128}\n",
        "mixed_precision   = \"no\"  # @param [\"no\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "skip_existing     = False  # @param{type:\"boolean\"}\n",
        "flip_aug          = True  # @param{type:\"boolean\"}\n",
        "# @markdown Use `clean_caption` option to clean such as duplicate tags, `women` to `girl`, etc\n",
        "clean_caption     = True #@param {type:\"boolean\"}\n",
        "#@markdown Use the `recursive` option to process subfolders as well\n",
        "recursive         = False #@param {type:\"boolean\"}\n",
        "\n",
        "metadata_config = {\n",
        "    \"_train_data_dir\": filtered_dir,\n",
        "    \"_out_json\": metadata_json,\n",
        "    \"recursive\": recursive,\n",
        "    \"full_path\": recursive,\n",
        "    \"clean_caption\": clean_caption\n",
        "}\n",
        "\n",
        "bucketing_config = {\n",
        "    \"_train_data_dir\": filtered_dir,\n",
        "    \"_in_json\": metadata_json,\n",
        "    \"_out_json\": bucketing_json,\n",
        "    \"_model_name_or_path\": vae_path if vae_path else model_path,\n",
        "    \"recursive\": recursive,\n",
        "    \"full_path\": recursive,\n",
        "    \"flip_aug\": flip_aug,\n",
        "    \"skip_existing\": skip_existing,\n",
        "    \"batch_size\": 4,\n",
        "    \"max_data_loader_n_workers\": 2,\n",
        "    \"max_resolution\": f\"{bucket_resolution}, {bucket_resolution}\",\n",
        "    \"mixed_precision\": mixed_precision,\n",
        "}\n",
        "\n",
        "def generate_args(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "    return args.strip()\n",
        "\n",
        "merge_metadata_args = generate_args(metadata_config)\n",
        "prepare_buckets_args = generate_args(bucketing_config)\n",
        "\n",
        "merge_metadata_command = f\"python merge_all_to_metadata.py {merge_metadata_args}\"\n",
        "prepare_buckets_command = f\"python prepare_buckets_latents.py {prepare_buckets_args}\"\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "!{merge_metadata_command}\n",
        "time.sleep(1)\n",
        "!{prepare_buckets_command}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0F6Qwp1hI1Og"
      },
      "outputs": [],
      "source": [
        "#@title Zip Explicit Dataset\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_type = \"explicit\" #@param [\"explicit\", \"safe\", \"suggestive\"]\n",
        "source_folder = \"/content/filtered/\" #@param {type:\"string\"}\n",
        "source_folder_to_zip = source_folder + zip_type\n",
        "output_dir = \"/content/LoRA\" #@param {type:\"string\"}\n",
        "output_name = zip_type\n",
        "#zip_part = \"[x]\" #@param {type:\"string\"}\n",
        "\n",
        "if output_name:\n",
        "  output_name = part_data + \"_\" + zip_type\n",
        "else:\n",
        "  output_name = prefix_to_add\n",
        "\n",
        "output_path_for_zip = f'{output_dir}/{output_name}.zip'\n",
        "explicit_dataset_zip = output_path_for_zip\n",
        "\n",
        "def zip_folder_contents(folder_path, output_zip_path):\n",
        "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, folder_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    source_folder = source_folder_to_zip\n",
        "    output_zip = output_path_for_zip\n",
        "\n",
        "    zip_folder_contents(source_folder, output_zip)\n",
        "    print(\"Folder contents compressed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ms8CVqE-F4pN"
      },
      "outputs": [],
      "source": [
        "#@title Push Explicit Zip to Huggingface\n",
        "\n",
        "import os\n",
        "from huggingface_hub import upload_file, create_repo\n",
        "\n",
        "file_path = \"\" #@param {type:\"string\"}\n",
        "prefix = \"\" #@param {type:\"string\"}\n",
        "\n",
        "hf_token = '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' #@param {type:\"string\"}\n",
        "repo_id = 'Alterneko/a' #@param {type:\"string\"}\n",
        "repo_type = 'dataset' #@param ['model','dataset']\n",
        "commit_message = 'Upload' #@param {type:\"string\"}\n",
        "make_private = False #@param {type:\"boolean\"}\n",
        "\n",
        "version_data = \"v3\" #@param {type:\"string\"}\n",
        "zip_type = \"processed\" #@param ['raw','processed','bucket']\n",
        "\n",
        "if file_path == \"\":\n",
        "  file_path = explicit_dataset_zip\n",
        "else:\n",
        "  file_path = file_path\n",
        "\n",
        "if commit_message:\n",
        "  commit_message = commit_message\n",
        "else:\n",
        "  commit_message = f'Push {filename}'\n",
        "\n",
        "if version_data:\n",
        "  version = f'{version_data}/'\n",
        "else:\n",
        "  version = \"\"\n",
        "\n",
        "model_path = file_path\n",
        "filename = os.path.basename(model_path)\n",
        "final_filename = f'{filename}'\n",
        "path_in_repo = f'{zip_type}/{version}{final_filename}'\n",
        "\n",
        "create_repo(repo_id=repo_id, repo_type=repo_type, token=hf_token, exist_ok=True, private=make_private)\n",
        "upload_file(path_or_fileobj=model_path, path_in_repo=path_in_repo, repo_id=repo_id, repo_type=repo_type, commit_message=commit_message, token=hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cBMLadDtJs2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Push to HF.co\n",
        "\n",
        "import os\n",
        "from huggingface_hub import upload_file, create_repo\n",
        "\n",
        "file_path = \"/content/meta/explicit_meta_lat.json\" #@param {type:\"string\"}\n",
        "hf_token = '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' #@param {type:\"string\"}\n",
        "repo_id = 'Alterneko/a' #@param {type:\"string\"}\n",
        "repo_type = 'dataset' #@param ['model','dataset']\n",
        "commit_message = 'Update' #@param {type:\"string\"}\n",
        "make_private = False #@param {type:\"boolean\"}\n",
        "\n",
        "version_data = \"v3\" #@param {type:\"string\"}\n",
        "zip_type = \"bucket\" #@param ['raw','processed','bucket']\n",
        "\n",
        "if commit_message:\n",
        "  commit_message = commit_message\n",
        "else:\n",
        "  commit_message = f'Push {filename}'\n",
        "\n",
        "if version_data:\n",
        "  version = f'{version_data}/'\n",
        "else:\n",
        "  version = \"\"\n",
        "\n",
        "model_path = file_path\n",
        "filename = os.path.basename(model_path)\n",
        "final_filename = f'{part_data}_{filename}'\n",
        "path_in_repo = f'{zip_type}/{version}{final_filename}'\n",
        "\n",
        "create_repo(repo_id=repo_id, repo_type=repo_type, token=hf_token, exist_ok=True, private=make_private)\n",
        "upload_file(path_or_fileobj=model_path, path_in_repo=path_in_repo, repo_id=repo_id, repo_type=repo_type, commit_message=commit_message, token=hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5QN6vUJMEdRw"
      },
      "outputs": [],
      "source": [
        "# @title ## **3.4. Bucketing and Latents Caching**\n",
        "%store -r\n",
        "\n",
        "data_type = \"suggestive\" #@param [\"explicit\", \"safe\", \"suggestive\"]\n",
        "\n",
        "%mkdir \"/content/meta/\"\n",
        "meta_dir = \"/content/meta/\"\n",
        "\n",
        "filtered_dir = \"/content/filtered/\" + data_type\n",
        "# @markdown This code will create buckets based on the `bucket_resolution` provided for multi-aspect ratio training, and then convert all images within the `train_data_dir` to latents.\n",
        "bucketing_json    = os.path.join(meta_dir, f\"{data_type}_meta_lat.json\")\n",
        "metadata_json     = os.path.join(meta_dir, f\"{data_type}_meta_clean.json\")\n",
        "bucket_resolution = 512  # @param {type:\"slider\", min:512, max:1024, step:128}\n",
        "mixed_precision   = \"no\"  # @param [\"no\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "skip_existing     = False  # @param{type:\"boolean\"}\n",
        "flip_aug          = True  # @param{type:\"boolean\"}\n",
        "# @markdown Use `clean_caption` option to clean such as duplicate tags, `women` to `girl`, etc\n",
        "clean_caption     = True #@param {type:\"boolean\"}\n",
        "#@markdown Use the `recursive` option to process subfolders as well\n",
        "recursive         = False #@param {type:\"boolean\"}\n",
        "\n",
        "metadata_config = {\n",
        "    \"_train_data_dir\": filtered_dir,\n",
        "    \"_out_json\": metadata_json,\n",
        "    \"recursive\": recursive,\n",
        "    \"full_path\": recursive,\n",
        "    \"clean_caption\": clean_caption\n",
        "}\n",
        "\n",
        "bucketing_config = {\n",
        "    \"_train_data_dir\": filtered_dir,\n",
        "    \"_in_json\": metadata_json,\n",
        "    \"_out_json\": bucketing_json,\n",
        "    \"_model_name_or_path\": vae_path if vae_path else model_path,\n",
        "    \"recursive\": recursive,\n",
        "    \"full_path\": recursive,\n",
        "    \"flip_aug\": flip_aug,\n",
        "    \"skip_existing\": skip_existing,\n",
        "    \"batch_size\": 4,\n",
        "    \"max_data_loader_n_workers\": 2,\n",
        "    \"max_resolution\": f\"{bucket_resolution}, {bucket_resolution}\",\n",
        "    \"mixed_precision\": mixed_precision,\n",
        "}\n",
        "\n",
        "def generate_args(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "    return args.strip()\n",
        "\n",
        "merge_metadata_args = generate_args(metadata_config)\n",
        "prepare_buckets_args = generate_args(bucketing_config)\n",
        "\n",
        "merge_metadata_command = f\"python merge_all_to_metadata.py {merge_metadata_args}\"\n",
        "prepare_buckets_command = f\"python prepare_buckets_latents.py {prepare_buckets_args}\"\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "!{merge_metadata_command}\n",
        "time.sleep(1)\n",
        "!{prepare_buckets_command}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "te0Dy07SJfN8"
      },
      "outputs": [],
      "source": [
        "#@title Zip Suggestive Dataset\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_type = \"suggestive\" #@param [\"explicit\", \"safe\", \"suggestive\"]\n",
        "source_folder = \"/content/filtered/\" #@param {type:\"string\"}\n",
        "source_folder_to_zip = source_folder + zip_type\n",
        "output_dir = \"/content/LoRA\" #@param {type:\"string\"}\n",
        "output_name = zip_type\n",
        "zip_part = \"[x]\" #@param {type:\"string\"}\n",
        "\n",
        "if output_name:\n",
        "  output_name = part_data + \"_\" + zip_type\n",
        "else:\n",
        "  output_name = prefix_to_add\n",
        "\n",
        "output_path_for_zip = f'{output_dir}/{output_name}.zip'\n",
        "suggestive_dataset_zip = output_path_for_zip\n",
        "\n",
        "def zip_folder_contents(folder_path, output_zip_path):\n",
        "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, folder_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    source_folder = source_folder_to_zip\n",
        "    output_zip = output_path_for_zip\n",
        "\n",
        "    zip_folder_contents(source_folder, output_zip)\n",
        "    print(\"Folder contents compressed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NhNqSzjBGBzm"
      },
      "outputs": [],
      "source": [
        "#@title Push Suggestive Zip to Huggingface\n",
        "\n",
        "import os\n",
        "from huggingface_hub import upload_file, create_repo\n",
        "\n",
        "file_path = \"\" #@param {type:\"string\"}\n",
        "prefix = \"\" #@param {type:\"string\"}\n",
        "\n",
        "hf_token = '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' #@param {type:\"string\"}\n",
        "repo_id = 'Alterneko/a' #@param {type:\"string\"}\n",
        "repo_type = 'dataset' #@param ['model','dataset']\n",
        "commit_message = 'Upload' #@param {type:\"string\"}\n",
        "make_private = False #@param {type:\"boolean\"}\n",
        "\n",
        "version_data = \"v3\" #@param {type:\"string\"}\n",
        "zip_type = \"processed\" #@param ['raw','processed','bucket']\n",
        "\n",
        "if file_path == \"\":\n",
        "  file_path = suggestive_dataset_zip\n",
        "else:\n",
        "  file_path = file_path\n",
        "\n",
        "if commit_message:\n",
        "  commit_message = commit_message\n",
        "else:\n",
        "  commit_message = f'Push {filename}'\n",
        "\n",
        "if version_data:\n",
        "  version = f'{version_data}/'\n",
        "else:\n",
        "  version = \"\"\n",
        "\n",
        "model_path = file_path\n",
        "filename = os.path.basename(model_path)\n",
        "final_filename = f'{filename}'\n",
        "path_in_repo = f'{zip_type}/{version}{final_filename}'\n",
        "\n",
        "create_repo(repo_id=repo_id, repo_type=repo_type, token=hf_token, exist_ok=True, private=make_private)\n",
        "upload_file(path_or_fileobj=model_path, path_in_repo=path_in_repo, repo_id=repo_id, repo_type=repo_type, commit_message=commit_message, token=hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0Y-cnsASKZbV"
      },
      "outputs": [],
      "source": [
        "#@title Push to HF.co\n",
        "\n",
        "import os\n",
        "from huggingface_hub import upload_file, create_repo\n",
        "\n",
        "file_path = \"/content/meta/suggestive_meta_lat.json\" #@param {type:\"string\"}\n",
        "\n",
        "hf_token = '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' #@param {type:\"string\"}\n",
        "repo_id = 'Alterneko/a' #@param {type:\"string\"}\n",
        "repo_type = 'dataset' #@param ['model','dataset']\n",
        "commit_message = 'Update' #@param {type:\"string\"}\n",
        "make_private = False #@param {type:\"boolean\"}\n",
        "\n",
        "version_data = \"v3\" #@param {type:\"string\"}\n",
        "zip_type = \"bucket\" #@param ['raw','processed','bucket']\n",
        "\n",
        "if commit_message:\n",
        "  commit_message = commit_message\n",
        "else:\n",
        "  commit_message = f'Push {filename}'\n",
        "\n",
        "if version_data:\n",
        "  version = f'{version_data}/'\n",
        "else:\n",
        "  version = \"\"\n",
        "\n",
        "model_path = file_path\n",
        "filename = os.path.basename(model_path)\n",
        "final_filename = f'{part_data}_{filename}'\n",
        "path_in_repo = f'{zip_type}/{version}{final_filename}'\n",
        "\n",
        "create_repo(repo_id=repo_id, repo_type=repo_type, token=hf_token, exist_ok=True, private=make_private)\n",
        "upload_file(path_or_fileobj=model_path, path_in_repo=path_in_repo, repo_id=repo_id, repo_type=repo_type, commit_message=commit_message, token=hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2TmetROcZ1nG"
      },
      "outputs": [],
      "source": [
        "#@title Zip lowres Folder\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "source_folder = \"/content/LoRA/lowres_Data\" #@param {type:\"string\"}\n",
        "source_folder_to_zip = source_folder\n",
        "output_dir = \"/content/LoRA\" #@param {type:\"string\"}\n",
        "output_name = \"lowres\" #@param {type:\"string\"}\n",
        "#zip_part = \"[x]\" #@param {type:\"string\"}\n",
        "\n",
        "if output_name:\n",
        "  output_name = output_name + \"_\" + part_data\n",
        "else:\n",
        "  output_name = prefix_to_add\n",
        "\n",
        "output_path_for_zip = f'{output_dir}/{output_name}.zip'\n",
        "lowres_dataset_zip = output_path_for_zip\n",
        "\n",
        "def zip_folder_contents(folder_path, output_zip_path):\n",
        "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, folder_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    source_folder = source_folder_to_zip\n",
        "    output_zip = output_path_for_zip\n",
        "\n",
        "    zip_folder_contents(source_folder, output_zip)\n",
        "    print(\"Folder contents compressed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5mkgJ0UmEn-3"
      },
      "outputs": [],
      "source": [
        "#@title Push Lowres Zip to HF.co\n",
        "\n",
        "import os\n",
        "from huggingface_hub import upload_file, create_repo\n",
        "\n",
        "file_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "prefix = \"\" #@param {type:\"string\"}\n",
        "\n",
        "hf_token = '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' #@param {type:\"string\"}\n",
        "repo_id = 'Alterneko/a' #@param {type:\"string\"}\n",
        "repo_type = 'dataset' #@param ['model','dataset']\n",
        "commit_message = 'Upload' #@param {type:\"string\"}\n",
        "make_private = False #@param {type:\"boolean\"}\n",
        "\n",
        "version_data = \"v3\" #@param {type:\"string\"}\n",
        "zip_type = \"processed\" #@param ['raw','processed','bucket']\n",
        "\n",
        "if file_path == \"\":\n",
        "  file_path = lowres_dataset_zip\n",
        "else:\n",
        "  file_path = file_path\n",
        "\n",
        "if commit_message:\n",
        "  commit_message = commit_message\n",
        "else:\n",
        "  commit_message = f'Push {filename}'\n",
        "\n",
        "if version_data:\n",
        "  version = f'{version_data}/'\n",
        "else:\n",
        "  version = \"\"\n",
        "\n",
        "model_path = file_path\n",
        "filename = os.path.basename(model_path)\n",
        "final_filename = f'{prefix}_{filename}'\n",
        "path_in_repo = f'{zip_type}/{version}{final_filename}'\n",
        "\n",
        "create_repo(repo_id=repo_id, repo_type=repo_type, token=hf_token, exist_ok=True, private=make_private)\n",
        "upload_file(path_or_fileobj=model_path, path_in_repo=path_in_repo, repo_id=repo_id, repo_type=repo_type, commit_message=commit_message, token=hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AVuLjWQ0py3e"
      },
      "outputs": [],
      "source": [
        "#@title Clear Folder\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def clear_folder_contents(folder_path):\n",
        "    for item in os.listdir(folder_path):\n",
        "        item_path = os.path.join(folder_path, item)\n",
        "        if os.path.isfile(item_path):\n",
        "            os.unlink(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)\n",
        "\n",
        "# Ganti 'folder_path' dengan jalur folder yang ingin kamu hapus isinya\n",
        "folder_path = '/content/LoRA/train_data' #@param {type:\"string\"}\n",
        "clear_folder_contents(folder_path)\n",
        "print(f\"Isi folder {folder_path} telah dihapus.\")\n",
        "print(f\"All Done!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}